{"searchDocs":[{"title":"Overview","type":0,"sectionRef":"#","url":"/terraform-databricks-sra/docs/overview/","content":"","keywords":"","version":"Next"},{"title":"Point-in-Time Solution‚Äã","type":1,"pageTitle":"Overview","url":"/terraform-databricks-sra/docs/overview/#point-in-time-solution","content":" The Security Reference Architecture (SRA) - Terraform Templates is designed as a point-in-time solution that captures security best practices at the time of each release. This project does not guarantee backward compatibility between versions; new releases are not drop-in replacements for existing codebases.  ","version":"Next","tagName":"h2"},{"title":"Project Support‚Äã","type":1,"pageTitle":"Overview","url":"/terraform-databricks-sra/docs/overview/#project-support","content":" The code in this project is provided for exploration purposes only and is not formally supported by Databricks under any Service Level Agreements (SLAs). It is provided AS-IS, without any warranties or guarantees. Please do not submit support tickets to Databricks for issues related to the use of this project. The source code provided is subject to the Databricks LICENSE . All third-party libraries included or referenced are subject to their respective licenses set forth in the project license. Any issues or bugs found should be submitted as GitHub Issues on the project repository. While these will be reviewed as time permits, there are no formal SLAs for support. ","version":"Next","tagName":"h2"},{"title":"Usage","type":0,"sectionRef":"#","url":"/terraform-databricks-sra/docs/usage/","content":"Usage Note For feedback, questions, and comments ‚Äî please open a GitHub Issue. Please review the Project Support section for important information on support and service terms. The Security Reference Architecture (SRA) can be used to deploy Databricks on any of the supported cloud platforms: AWS/AWS GovCloudAzureGCP Please follow the usage guide specific to your cloud provider.","keywords":"","version":"Next"},{"title":"Additional Resources","type":0,"sectionRef":"#","url":"/terraform-databricks-sra/docs/additionalresources/","content":"Additional Resources Learn more about Databricks Security and Trust best practices and the Security Reference Architecture (SRA) through these resources. üõ°Ô∏è Databricks Security Databricks Security &amp; Trust CenterSecurity Best Practices for Databricks on AWSSecurity Best Practices for Azure DatabricksSecurity Best Practices for Databricks on GCPDatabricks Security Best Practices YouTube SeriesSecurity Analysis Tool (SAT) üìù Databricks Platform Blog Posts A Unified Approach to Data Exfiltration Protection on DatabricksData Exfiltration Protection with Databricks Lakehouse Platform on AWSData Exfiltration Protection with Azure DatabricksDatabricks on GCP - A practitioners guide on data exfiltration protections üîó Useful Links Databricks DocumentationDatabricks Platform SME Medium PageAdditional Databricks Terraform ExamplesDatabricks Terraform Provider DocumentationTerraform Documentation","keywords":"","version":"Next"},{"title":"Getting Started","type":0,"sectionRef":"#","url":"/terraform-databricks-sra/docs/usage/AWS/gettingstarted/","content":"","keywords":"","version":"Next"},{"title":"SRA Installation and Deployment Steps‚Äã","type":1,"pageTitle":"Getting Started","url":"/terraform-databricks-sra/docs/usage/AWS/gettingstarted/#sra-installation-and-deployment-steps","content":" Follow the steps below to deploy the Security Reference Architecture (SRA) using Terraform:  Clone the SRA repository.Install Terraform.Navigate to the aws -&gt; tf folder and open up the template.tfvars.example file. Fill in the required values for all of the variables and relevant feature flags that are required for the deployment.NOTE: If using custom mode for the network configuration, do not uncomment the variables for the isolated configuration. Simply uncomment the variables for the custom configuration and populate with the respective values for each.Rename the file to terraform.tfvars. Navigate to the provider.tf file and configure the AWS and Databricks Terraform provider authentication.From the terminal, ensure you are in the correct working directory for the tf folder.Run terraform init.Run terraform validate.Run terraform plan.Run terraform apply.  ","version":"Next","tagName":"h2"},{"title":"Critical Next Steps‚Äã","type":1,"pageTitle":"Getting Started","url":"/terraform-databricks-sra/docs/usage/AWS/gettingstarted/#critical-next-steps","content":" The following steps outline essential security configurations that should be implemented after the initial deployment to further harden and operationalize the Databricks environment.  Implement a Front-End Mitigation Strategy: IP Access Lists: Terraform configurations for enabling IP access lists are available in the customizations folder.Front-End PrivateLink: Establishes a private connection to the Databricks web application over the AWS backbone, preventing exposure to the public internet. Read the documentation here. Identity &amp; Access Management: Configure Single-Sign On and Multi-Factor Authentication: Enterprise deployments should implement SSO and MFA for secure authentication and identity management.Setup SCIM (System for Cross-domain Identity Management) Provisioning: For automated user and group provisioning, integrate SCIM through the Databricks account console.  ","version":"Next","tagName":"h2"},{"title":"Additional Security Recommendations‚Äã","type":1,"pageTitle":"Getting Started","url":"/terraform-databricks-sra/docs/usage/AWS/gettingstarted/#additional-security-recommendations","content":" The following recommendations help maintain a strong security posture across Databricks deployments. Some of these configurations extend beyond the SRA Terraform implementation and may require customer-specific setup (e.g., SCIM, SSO, or Front-End PrivateLink).  Segment Workspaces for Data Separation: Use distinct workspaces for different teams or functions (e.g., security, marketing) to enforce data access boundaries and reduce risk exposure.Avoid Storing Production Datasets in Databricks File Store (DBFS): The DBFS root is accessible to all users in a workspace. Use external storage locations for production data and databases to ensure proper access control and auditing.Back Up Assets from the Databricks Control Plane: Regularly export and back up notebooks, jobs, and configurations using tools such as the Databricks Terraform Exporter.Regularly Restart Classic Compute Clusters: Restart clusters periodically to ensure the latest compute images and security patches are applied.Integrate CI/CD and Code Management: Evaluate workflow needs for Git-based version control and CI/CD automation. Incorporate code scanning, permission enforcement, and secret detection to enhance governance and operational efficiency.Deploy and Run the Security Analyis Tool (SAT): SAT analyzes your Databricks account and workspace configurations, providing recommendations to help you follow Databricks' security best practices. ","version":"Next","tagName":"h2"},{"title":"Getting Started","type":0,"sectionRef":"#","url":"/terraform-databricks-sra/docs/usage/Azure/gettingstarted/","content":"","keywords":"","version":"Next"},{"title":"SRA Installation and Deployment Steps‚Äã","type":1,"pageTitle":"Getting Started","url":"/terraform-databricks-sra/docs/usage/Azure/gettingstarted/#sra-installation-and-deployment-steps","content":" Follow the steps below to deploy the Security Reference Architecture (SRA) using Terraform:  Clone the SRA repository. Install Terraform. Navigate to the azure -&gt; tf folder and choose the appropriate template file based on your deployment mode: Mode 1 - Full SRA-managed: template.example.tfvars (default, creates hub and spoke)Mode 2 - Bring-your-own hub: template_byo_hub.example.tfvars (uses existing hub infrastructure)Mode 3 - Bring-your-own hub and spoke: template_byo_spoke_network.example.tfvars (uses existing hub and spoke networks) See Deployment Modes for detailed guidance on choosing the right mode for your environment. Fill in the required values for all of the variables and relevant feature flags that are required for the deployment.Rename the file to terraform.tfvars. From the terminal, ensure you are in the correct working directory for the tf folder. Run terraform init. Run terraform plan. Run terraform apply.  If you encounter errors during any of the above terraform steps, check the troubleshooting guide.  ","version":"Next","tagName":"h2"},{"title":"Critical Next Steps‚Äã","type":1,"pageTitle":"Getting Started","url":"/terraform-databricks-sra/docs/usage/Azure/gettingstarted/#critical-next-steps","content":" The following steps outline essential security configurations that should be implemented after the initial deployment to further harden and operationalize the Databricks environment.  Implement a Front-End Mitigation Strategy: IP Access Lists: IP Access Lists enhance security by providing control over which networks can connect to your Azure Databricks account and workspaces.Front-End PrivateLink: Front-End PrivateLink establishes a private connection to the Databricks web application over the Azure backbone, preventing exposure to the public internet.  ","version":"Next","tagName":"h2"},{"title":"Additional Security Recommendations‚Äã","type":1,"pageTitle":"Getting Started","url":"/terraform-databricks-sra/docs/usage/Azure/gettingstarted/#additional-security-recommendations","content":" Segment Workspaces for Data Separation: Use distinct workspaces for different teams or functions (e.g., security, marketing) to enforce data access boundaries and reduce risk exposure.Avoid Storing Production Datasets in Databricks File Store (DBFS): The DBFS root is accessible to all users in a workspace. Use external storage locations for production data and databases to ensure proper access control and auditing.Back Up Assets from the Databricks Control Plane: Regularly export and back up notebooks, jobs, and configurations using tools such as the Databricks Terraform Exporter.Implement a Tagging Strategy: Cluster and pool tags enable organizations to monitor costs and accurately attribute Databricks usage to specific business units or teams. These tags propagate to detailed DBU usage reports, supporting cost analysis and internal chargeback processes.Integrate CI/CD and Code Management: Evaluate workflow needs for Git-based version control and CI/CD automation. Incorporate code scanning, permission enforcement, and secret detection to enhance governance and operational efficiency.Run and Monitor the Security Analysis Tool (SAT): SAT analyzes your Databricks account and workspace configurations, providing recommendations to help you follow Databricks' security best practices. ","version":"Next","tagName":"h2"},{"title":"AWS GovCloud","type":0,"sectionRef":"#","url":"/terraform-databricks-sra/docs/usage/AWS/awsgovcloud/","content":"","keywords":"","version":"Next"},{"title":"Overview‚Äã","type":1,"pageTitle":"AWS GovCloud","url":"/terraform-databricks-sra/docs/usage/AWS/awsgovcloud/#overview","content":" Databricks supports deployments in AWS GovCloud (US) regions to meet the compliance, security, and data residency requirements of U.S. government agencies and contractors. The Security Reference Architecture (SRA) Terraform templates have been extended to support GovCloud-specific configurations while maintaining the same security-first design principles as commercial regions.  GovCloud deployments provide a controlled environment that adheres to U.S. government security and regulatory standards such as FedRAMP High and DoD IL5. These deployments ensure that all data and metadata remain within the U.S., operated exclusively by screened U.S. personnel.  ","version":"Next","tagName":"h2"},{"title":"Configuration Requirements‚Äã","type":1,"pageTitle":"AWS GovCloud","url":"/terraform-databricks-sra/docs/usage/AWS/awsgovcloud/#configuration-requirements","content":" In addition to the steps outlined in the Getting Started page, the following parameters must be defined in the Terraform configuration (.tfvars) when deploying in a GovCloud region:  Region: Set region to us-gov-west-1.GovCloud Shard: Set databricks_gov_shard to either civilian or dod. Use civilian for most U.S. government agency workloads.Use dod for Department of Defense (DoD) environments. NOTE: The dod shard is restricted to customers with a .mil email address.  For all non-GovCloud (commercial) deployments, leave databricks_gov_shard set to null.  ","version":"Next","tagName":"h2"},{"title":"Networking and Access Considerations‚Äã","type":1,"pageTitle":"AWS GovCloud","url":"/terraform-databricks-sra/docs/usage/AWS/awsgovcloud/#networking-and-access-considerations","content":" Private Connectivity: GovCloud workspaces can be deployed with full private connectivity through AWS PrivateLink, ensuring no traffic traverses the public internet from the classic compute plane.Region Isolation: All data and metadata remain within the designated GovCloud region, ensuring compliance with government data residency requirements.IAM and Policy Controls: Follow the same IAM and endpoint policy guidance as commercial SRA deployments, while aligning with agency-specific security baselines.  ","version":"Next","tagName":"h2"},{"title":"Additional Resources‚Äã","type":1,"pageTitle":"AWS GovCloud","url":"/terraform-databricks-sra/docs/usage/AWS/awsgovcloud/#additional-resources","content":" Databricks on AWS GovCloud DocumentationCompliance Security Profile and Enhanced Security Monitoring ","version":"Next","tagName":"h2"},{"title":"SRA Components Breakdown","type":0,"sectionRef":"#","url":"/terraform-databricks-sra/docs/usage/AWS/components/","content":"","keywords":"","version":"Next"},{"title":"Network Configuration‚Äã","type":1,"pageTitle":"SRA Components Breakdown","url":"/terraform-databricks-sra/docs/usage/AWS/components/#network-configuration","content":" Two network configurations are available for workspaces: Isolated or Custom:  Isolated (Default): Restricts all traffic from reaching the public internet. Communication is limited to AWS PrivateLink endpoints for AWS services and the Databricks control plane. Note: A Unity Catalog‚Äìonly configuration is required for clusters running without internet access. Read the offical documentation for details. Custom: Enables specification of a VPC ID, subnet IDs, security group IDs, and Databricks PrivateLink endpoint IDs. This option is recommended when networking assets are provisioned in separate pipelines or pre-assigned by a centralized infrastructure team.  ","version":"Next","tagName":"h2"},{"title":"Core AWS Components‚Äã","type":1,"pageTitle":"SRA Components Breakdown","url":"/terraform-databricks-sra/docs/usage/AWS/components/#core-aws-components","content":" Customer Managed VPC: A customer-managed VPC provides full control over network configurations to meet organizational cloud security and governance standards.S3 Buckets: Three S3 buckets are created to support the following functionalities: Workspace Root BucketUnity Catalog - Workspace CatalogAudit Log Delivery Bucket IAM Roles: Three IAM roles are created to support the following functionalities: Classic Compute (EC2) ProvisioningData Access for Unity Catalog - Workspace CatalogAudit Log Delivery Scoped-down IAM Policy for the Databricks Cross-Account Role: A cross-account role is required for clusters provisioned within the classic compute plane. The role is scoped to the VPC, subnets, and security group associated with the deployment.AWS VPC Endpoints for S3 Gateway, STS, and Kinesis: AWS PrivateLink is used to connect the VPC to AWS services without traversing public IP addresses. S3 Gateway, STS, and Kinesis endpoints are best practices for enterprise Databricks deployments. Additional endpoints such as those for AWS DynamoDB and AWS Glue can be configured based on your use cases. NOTE: In the Isolated network mode, restrictive VPC endpoint policies are applied for S3, STS, and Kinesis. These must be updated if additional access is required through the classic compute plane. Back-end AWS PrivateLink Connectivity: Ensures private communication between the classic compute plane and the Databricks control plane (Back-end PrivateLink) via Databricks-specific interface VPC endpoints. Front-end PrivateLink, which keeps user traffic on the AWS backbone, is available but not included in these templates.AWS KMS Keys: Three AWS KMS Keys are created for: Workspace StorageManaged ServicesUnity Catalog - Workspace Catalog  ","version":"Next","tagName":"h2"},{"title":"Core Databricks Components‚Äã","type":1,"pageTitle":"SRA Components Breakdown","url":"/terraform-databricks-sra/docs/usage/AWS/components/#core-databricks-components","content":" Unity Catalog: Unity Catalog is a unified governance solution for data and AI assets such as files, tables, and machine learning models. Unity Catalog enforces fine-grained access controls, centralized policy management, auditing, and lineage tracking‚Äîall integrated into the Databricks workflow.System Table Schemas: System Tables provide operational visibility across access, compute, Lakeflow, query, serving, and storage logs. These tables are located within the system catalog in Unity Catalog.Audit Log Delivery: Enables low-latency delivery of Databricks audit logs to an S3 bucket within the customer's AWS account. Audit Logs capture both workspace-level and account-level events, with an option to enable verbose logging for more detailed event data. NOTE: Audit log delivery can only be configured twice per account. Once enabled, set audit_log_delivery_exists = true for subsequent runs. Network Connectivity Configuration: Serverless network connectivity is managed with network connectivity configurations (NCC), which are account-level regional constructs that are used to manage private endpoints creation and firewall enablement at scale. An NCC is created and attached to the workspace, which contains a list of stable IP addresses, which will be used by the serverless compute in that workspace to connect to customer cloud resources.Restrictive Network Policy: Network Policies implement egress controls for serverless compute by enforcing a restrictive network policy that permits outbound traffic only to required data buckets.Example Classic Cluster: Includes a sample cluster and associated cluster policy to illustrate secure configuration patterns. NOTE: Deploying this example creates a cluster within the Databricks workspace, including the underlying AWS EC2 instance. ","version":"Next","tagName":"h2"},{"title":"Read Before Deploying","type":0,"sectionRef":"#","url":"/terraform-databricks-sra/docs/usage/AWS/preread/","content":"","keywords":"","version":"Next"},{"title":"Key Considerations‚Äã","type":1,"pageTitle":"Read Before Deploying","url":"/terraform-databricks-sra/docs/usage/AWS/preread/#key-considerations","content":" The Security Reference Architecture (SRA) is a purpose-built, simplified deployment pattern designed for highly secure and regulated customers.  This architecture includes specific functionalities that may affect certain use cases, as outlined below.  No outbound internet traffic: There is no outbound internet access from the classic compute plane or serverless compute plane. To add packages to classic compute or serverless compute, set up a private repository for scanned packages.Consider using a modern firewall solution to connect to public API endpoints if public internet connectivity is required. Restrictive AWS Resource Policies: Restrictive bucket and endpoint policies have been implemented for the workspace root storage bucket, S3 gateway endpoint, and the STS and Kinesis interface endpoints. These restrictions are continuously refined as the product evolves. Policies can be adjusted to allow access to additional AWS resources, such as other S3 buckets.If you encounter unexpected product behavior due to a policy in this repository, please raise a GitHub issue. Isolated Unity Catalog Securables: Unity Catalog securables like catalogs, Storage Credentials, and External Locations are isolated to individual workspaces. To share securables between workspaces, update the resources using the databricks_workspace_binding resource.  ","version":"Next","tagName":"h2"},{"title":"Customizations‚Äã","type":1,"pageTitle":"Read Before Deploying","url":"/terraform-databricks-sra/docs/usage/AWS/preread/#customizations","content":" Terraform customizations are available to support the baseline deployment of the Security Reference Architecture (SRA). These customizations are organized by provider:  Workspace: Databricks workspace provider  These extensions can be found in the top-level customizations folder. ","version":"Next","tagName":"h2"},{"title":"AWS","type":0,"sectionRef":"#","url":"/terraform-databricks-sra/docs/usage/AWS/","content":"","keywords":"","version":"Next"},{"title":"Overview‚Äã","type":1,"pageTitle":"AWS","url":"/terraform-databricks-sra/docs/usage/AWS/#overview","content":" Note For feedback, questions, and comments ‚Äî please open a GitHub Issue. Please review the Project Support section for important information on support and service terms.  The Security Reference Architecture (SRA) with Terraform provides a prescriptive deployment pattern for Databricks on AWS and AWS GovCloud, designed for highly secure and regulated environments. It captures Databricks security best practices in Terraform templates, allowing organizations to programmatically deploy workspaces and supporting infrastructure with hardened, opinionated defaults.  This architecture emphasizes:  Secure by Default ‚Äì Environments are provisioned with restrictive networking, scoped IAM roles, and encryption controls.Governance &amp; Compliance ‚Äì Aligns with industry standards and simplifies audits through consistent, automated configurations.Flexibility ‚Äì Templates can be customized to integrate with existing networking and security controls.Point-in-Time Design ‚Äì Each release reflects security best practices at that time; new releases may not be drop-in replacements.  ","version":"Next","tagName":"h2"},{"title":"Architecture Diagram‚Äã","type":1,"pageTitle":"AWS","url":"/terraform-databricks-sra/docs/usage/AWS/#architecture-diagram","content":"   The AWS implementation of the Security Reference Architecture (SRA) includes:  Configurable networking patterns (isolated or custom).Core AWS components such as VPCs and privatelink endpoints, IAM roles, S3 buckets, and KMS keys.Core Databricks components including Unity Catalog, system tables, audit log delivery, and network connectivty configurations and network policies for serverless compute.  ","version":"Next","tagName":"h2"},{"title":"Next Steps‚Äã","type":1,"pageTitle":"AWS","url":"/terraform-databricks-sra/docs/usage/AWS/#next-steps","content":" Review the Read Before Deploying section for critical considerations.Explore the SRA Components Breakdown to understand the included AWS and Databricks resources.Follow the Getting Started guide to deploy using Terraform. ","version":"Next","tagName":"h2"},{"title":"Read Before Deploying","type":0,"sectionRef":"#","url":"/terraform-databricks-sra/docs/usage/Azure/preread/","content":"","keywords":"","version":"Next"},{"title":"Key Considerations‚Äã","type":1,"pageTitle":"Read Before Deploying","url":"/terraform-databricks-sra/docs/usage/Azure/preread/#key-considerations","content":" The Security Reference Architecture (SRA) is a purpose-built, simplified deployment pattern designed for highly secure and regulated customers.  This architecture includes specific functionalities that may affect certain use cases, as outlined below.  Azure Firewall: Azure Firewall is deployed to securely manage and control outbound traffic from the classic compute plane. It provides centralized logging, monitoring, and policy enforcement while maintaining compliance with organizational security standards. To add packages (such as from PyPi or Cran) to classic compute or serverless compute, set up a private repository for scanned packages or update firewall rules/network policies to specified domains. See Network Egress Configuration for more information on configuring SRA for package installation. Hub-and-Spoke Network Topology: The architecture employs a hub-and-spoke model: The hub VNet contains shared infrastructure and services.The spoke Vnet houses the Azure Databricks workspace deployed by SRA. SRA can be used more than once for new workspaces for different business units or teams. This topology enhances security by isolating workloads and controlling traffic flow between VNets. Private Connectivity: Azure Databricks workspaces are deployed with VNet injection and Private Link, ensuring that all traffic between the workspace and Azure services remains within the Azure backbone network. Private endpoints are configured for services like Azure Storage, Event Hubs, and SQL Databases to prevent data exfiltration. Isolated Unity Catalog Securables: Unity Catalog securables like catalogs, Storage Credentials, and External Locations are isolated to individual workspaces. To share securables between workspaces, update the resources using the databricks_workspace_binding resource. Security Analysis Tool (SAT): The Security Analysis Tool (SAT) is disabled by default to support highly restricted no-egress deployments. When enabled, SAT is installed in the hub (webauth) workspace by default.SAT can be enabled by setting sat_configuration.enabled = true in your configuration.Requires specific network egress rules (see SAT URL Requirements).See the Components page for detailed configuration options.    ","version":"Next","tagName":"h2"},{"title":"Prerequisites‚Äã","type":1,"pageTitle":"Read Before Deploying","url":"/terraform-databricks-sra/docs/usage/Azure/preread/#prerequisites","content":" ","version":"Next","tagName":"h2"},{"title":"Required Permissions‚Äã","type":1,"pageTitle":"Read Before Deploying","url":"/terraform-databricks-sra/docs/usage/Azure/preread/#required-permissions","content":" The principal deploying SRA needs:  Azure Permissions:  Contributor role on the subscription. Alternatively, see this doc on creating a custom roleIf SRA is creating a service principal for SAT during deployment, you will need the permissions documented here.  Databricks Permissions:  Account admin permissions in the Databricks account  ","version":"Next","tagName":"h3"},{"title":"Authentication Setup‚Äã","type":1,"pageTitle":"Read Before Deploying","url":"/terraform-databricks-sra/docs/usage/Azure/preread/#authentication-setup","content":" To configure authentication for the Databricks provider, see the provider documentation here. ","version":"Next","tagName":"h3"},{"title":"Mode 2: Bring-your-own Hub","type":0,"sectionRef":"#","url":"/terraform-databricks-sra/docs/usage/Azure/mode2-byo-hub/","content":"","keywords":"","version":"Next"},{"title":"Description‚Äã","type":1,"pageTitle":"Mode 2: Bring-your-own Hub","url":"/terraform-databricks-sra/docs/usage/Azure/mode2-byo-hub/#description","content":" You provide an existing hub infrastructure (VNET, Key Vault, metastore, NCC, network policy). SRA creates only the spoke workspace with a managed spoke network.  ","version":"Next","tagName":"h2"},{"title":"What Gets Created‚Äã","type":1,"pageTitle":"Mode 2: Bring-your-own Hub","url":"/terraform-databricks-sra/docs/usage/Azure/mode2-byo-hub/#what-gets-created","content":" Resource\tComponent\tCreated by SRAHub Resource Group\tHub VNET + Azure Firewall\t‚úó (You provide) Webauth Workspace\t‚úó (You provide) CMK KeyVault\t‚úó (You provide) Route Table\t‚úó (You provide) Spoke Resource Group\tWorkspace\t‚úì Spoke VNET\t‚úì Back-end Private Endpoint\t‚úì UC Storage Account\t‚úì Account Console\tNCC (Network Connectivity Config)\t‚úó (You provide) Network Policy\t‚úó (You provide) Metastore\t‚úó (You provide)  ","version":"Next","tagName":"h2"},{"title":"Configuration‚Äã","type":1,"pageTitle":"Mode 2: Bring-your-own Hub","url":"/terraform-databricks-sra/docs/usage/Azure/mode2-byo-hub/#configuration","content":" create_hub = false   ","version":"Next","tagName":"h2"},{"title":"Required Variables‚Äã","type":1,"pageTitle":"Mode 2: Bring-your-own Hub","url":"/terraform-databricks-sra/docs/usage/Azure/mode2-byo-hub/#required-variables","content":" Additional Requirements When create_hub = false, you must provide several additional variables from your existing hub infrastructure.  # Disable hub creation create_hub = false # Basic configuration location = &quot;westus2&quot; subscription_id = &quot;ffffffff-ffff-ffff-ffff-ffffffffffff&quot; resource_suffix = &quot;spoke&quot; # REQUIRED: Existing metastore from your hub databricks_metastore_id = &quot;00000000-0000-0000-0000-000000000000&quot; # REQUIRED: Existing hub VNET details (for spoke network peering) existing_hub_vnet = { route_table_id = &quot;/subscriptions/.../providers/Microsoft.Network/routeTables/rt-hub&quot; vnet_id = &quot;/subscriptions/.../providers/Microsoft.Network/virtualNetworks/vnet-hub&quot; } # REQUIRED: Existing NCC and network policy existing_ncc_id = &quot;your-ncc-id&quot; existing_network_policy_id = &quot;your-network-policy-id&quot; # REQUIRED if cmk_enabled = true (default) existing_cmk_ids = { key_vault_id = &quot;/subscriptions/.../providers/Microsoft.KeyVault/vaults/kv-hub&quot; managed_disk_key_id = &quot;https://kv-hub.vault.azure.net/keys/cmk-disk/version&quot; managed_services_key_id = &quot;https://kv-hub.vault.azure.net/keys/cmk-services/version&quot; } tags = { Owner = &quot;user@example.com&quot; }   ","version":"Next","tagName":"h2"},{"title":"Required Variable Summary‚Äã","type":1,"pageTitle":"Mode 2: Bring-your-own Hub","url":"/terraform-databricks-sra/docs/usage/Azure/mode2-byo-hub/#required-variable-summary","content":" databricks_account_idlocationsubscription_idresource_suffixdatabricks_metastore_idexisting_ncc_idexisting_network_policy_idexisting_hub_vnetexisting_cmk_ids (if cmk_enabled = true)  ","version":"Next","tagName":"h3"},{"title":"Template File‚Äã","type":1,"pageTitle":"Mode 2: Bring-your-own Hub","url":"/terraform-databricks-sra/docs/usage/Azure/mode2-byo-hub/#template-file","content":" Use template_byo_hub.example.tfvars as the template for this type of deployment.  ","version":"Next","tagName":"h2"},{"title":"Important Notes‚Äã","type":1,"pageTitle":"Mode 2: Bring-your-own Hub","url":"/terraform-databricks-sra/docs/usage/Azure/mode2-byo-hub/#important-notes","content":" CMK Configuration If cmk_enabled = true (default), you must provide existing_cmk_ids. Alternatively, you can disable CMK: cmk_enabled = false   SAT Not Available The Security Analysis Tool (SAT) can only be deployed when create_hub = true. If you need SAT with a BYO hub deployment, you must deploy it separately.  ","version":"Next","tagName":"h2"},{"title":"Validation Rules‚Äã","type":1,"pageTitle":"Mode 2: Bring-your-own Hub","url":"/terraform-databricks-sra/docs/usage/Azure/mode2-byo-hub/#validation-rules","content":" When create_hub = false:  Must provide databricks_metastore_idMust provide existing_ncc_idMust provide existing_network_policy_idMust provide existing_hub_vnetMust provide existing_cmk_ids if cmk_enabled = trueMust NOT provide existing_cmk_ids if cmk_enabled = false  ","version":"Next","tagName":"h2"},{"title":"Next Steps‚Äã","type":1,"pageTitle":"Mode 2: Bring-your-own Hub","url":"/terraform-databricks-sra/docs/usage/Azure/mode2-byo-hub/#next-steps","content":" Copy template_byo_hub.example.tfvars to terraform.tfvarsFill in required variables from your existing hub infrastructureReview Configuration Reference for optional featuresFollow Getting Started deployment steps ","version":"Next","tagName":"h2"},{"title":"Read Before Deploying","type":0,"sectionRef":"#","url":"/terraform-databricks-sra/docs/usage/GCP/preread/","content":"","keywords":"","version":"Next"},{"title":"Key Considerations‚Äã","type":1,"pageTitle":"Read Before Deploying","url":"/terraform-databricks-sra/docs/usage/GCP/preread/#key-considerations","content":" The Security Reference Architecture (SRA) is a purpose-built, simplified deployment pattern designed for highly secure and regulated customers.  This architecture includes specific functionalities that may affect certain use cases, as outlined below.  Private Service Connect (PSC) Dependencies: This deployment implements back-end Private Service Connect (PSC) to ensure all workspace-to-control-plane traffic stays on the Google Cloud backbone. DNS records are automatically configured through Private Cloud DNS zones.The template does not include front-end PSC (for user-to-workspace traffic). If your security posture requires full private connectivity for user access, front-end PSC must be configured separately following the Databricks documentation. Customer-Managed VPC Configuration: The workspace is deployed into a Customer-Managed VPC to provide control over routing, firewall rules, and subnet segmentation. Ensure your organization‚Äôs security team reviews VPC peering or Shared VPC configurations before deployment.Adjust subnet CIDR ranges and routes as needed to align with your network design standards. Cloud KMS Integration: Customer-managed encryption keys (CMEK) are created via Cloud KMS and attached to both managed services and workspace storage.  ","version":"Next","tagName":"h2"},{"title":"Customizations‚Äã","type":1,"pageTitle":"Read Before Deploying","url":"/terraform-databricks-sra/docs/usage/GCP/preread/#customizations","content":" Terraform customizations are available to support the baseline deployment of the Security Reference Architecture (SRA).  These extensions and examples can be found in the top-level examples folder. ","version":"Next","tagName":"h2"},{"title":"SRA Components Breakdown","type":0,"sectionRef":"#","url":"/terraform-databricks-sra/docs/usage/GCP/components/","content":"","keywords":"","version":"Next"},{"title":"Core GCP Components‚Äã","type":1,"pageTitle":"SRA Components Breakdown","url":"/terraform-databricks-sra/docs/usage/GCP/components/#core-gcp-components","content":" Customer-Managed VPC: A Customer-Managed VPC provides full control over network configuration, routing, and segmentation. This setup is required for Private Service Connect (PSC) and enables centralized management of multiple workspaces within a single, compliant network environment.Back-End Private Service Connect (PSC): Establishes private connectivity between the Databricks control plane and the customer-managed VPC. This ensures that all control plane and compute plane communication remains on the Google Cloud backbone, eliminating public internet exposure. NOTE: Front-end PSC for user-to-workspace connectivity is not included in this deployment. Cloud KMS Keys (CMEK): Implements customer-managed encryption keys for both control plane‚Äìmanaged services (such as notebooks, secrets, and Databricks SQL query data) and workspace storage (including Cloud Storage buckets and GCE Persistent Disks).  After the workspace is created, the following components are provisioned:  Cloud DNS: Configures private DNS zones required for PSC to resolve internal Databricks service endpoints to private IPs within the VPC, maintaining isolation and secure communication between components. ","version":"Next","tagName":"h2"},{"title":"SRA Components Breakdown","type":0,"sectionRef":"#","url":"/terraform-databricks-sra/docs/usage/Azure/components/","content":"","keywords":"","version":"Next"},{"title":"Core Azure Components‚Äã","type":1,"pageTitle":"SRA Components Breakdown","url":"/terraform-databricks-sra/docs/usage/Azure/components/#core-azure-components","content":" Vnet Injection: Vnet Injection allows Azure Databricks workspaces to be deployed directly into a customer-managed virtual network (VNet), providing control over network configuration to meet organizational security and governance requirements.Private Endpoints: Leveraging Azure Private Link, private endpoints connect the customer‚Äôs VNet to Azure services without using public IP addresses, ensuring secure, private communication.PrivateLink Connectivity: Private Link establishes private network paths between the customer‚Äôs data plane and the Databricks control plane, preventing traffic from traversing the public internet. This template configures Back-End Private Link for communication to the Databricks control plane from classic compute clusters.  ","version":"Next","tagName":"h2"},{"title":"Core Databricks Components‚Äã","type":1,"pageTitle":"SRA Components Breakdown","url":"/terraform-databricks-sra/docs/usage/Azure/components/#core-databricks-components","content":" Unity Catalog: Unity Catalog is a unified governance solution for data and AI assets such as files, tables, and machine learning models. Unity Catalog enforces fine-grained access controls, centralized policy management, auditing, and lineage tracking‚Äîall integrated into the Databricks workflow.Network Connectivity Configuration: Serverless network connectivity is managed with network connectivity configurations (NCC), which are account-level regional constructs that are used to manage private endpoints creation and firewall enablement at scale. An NCC is created and attached to the workspace, which contains a list of stable Azure service subnets, which will be used by the serverless compute in that workspace to connect the Azure resource using service endpoints.Restrictive Network Policy: Network Policies implement egress controls for serverless compute by enforcing a restrictive network policy that permits outbound traffic only to required data buckets.  ","version":"Next","tagName":"h2"},{"title":"Security Analysis Tool (SAT)‚Äã","type":1,"pageTitle":"SRA Components Breakdown","url":"/terraform-databricks-sra/docs/usage/Azure/components/#security-analysis-tool-sat","content":" The Security Analysis Tool (SAT) is disabled by default. You can enable SAT to continuously monitor the security posture of your Databricks environment. By default, SAT is installed in the hub workspace when enabled, also known as the WEB_AUTH workspace.  Changing the SAT Workspace  To deploy the Security Analysis Tool (SAT) in a different workspace, three modifications are required in customizations.tf:  Update the Databricks provider in the SAT module:  # Default providers = { databricks = databricks.hub } # Modified providers = { databricks = databricks.spoke }   Update the local sat_workspace reference:  # Default locals { sat_workspace = var.create_hub &amp;&amp; length(module.webauth_workspace) &gt; 0 ? module.webauth_workspace[0] : null } # Modified locals { sat_workspace = module.spoke_workspace }   Update the databricks_permission_assignment.sat_workspace_admin resource  # Default resource &quot;databricks_permission_assignment&quot; &quot;sat_workspace_admin&quot; { count = length(module.sat) ... provider = databricks.hub } # Modified resource &quot;databricks_permission_assignment&quot; &quot;sat_workspace_admin&quot; { count = length(module.sat) ... provider = databricks.spoke }   NOTE: The Security Analysis Tool (SAT) is designed to be deployed once per Azure subscription.  SAT Service Principal  Some users may not have permissions to create Entra ID service principals. In this case, a pre-existing service principal can be used:  # example.tfvars sat_service_principal = { client_id = &quot;00000000-0000-0000-0000-000000000000&quot; client_secret = &quot;some-secret&quot; }   If no service principal is provided, the template creates one named spSAT by default. The name can be customized:  # example.tfvars sat_service_principal = { name = &quot;spSATDev&quot; }   SAT Compute  The Security Analysis Tool (SAT) is installed using classic compute by default. This is because SAT does not yet support inspecting workspaces outside of the current workspace when running on serverless. If you would like to run on serverless compute instead, you can modify the sat_configuration variable to specify using serverless (see below):  sat_configuration = { run_on_serverless = true }   NOTE: When running the Security Analysis Tool (SAT) on serverless compute, SAT will only inspect the current workspace. ","version":"Next","tagName":"h2"},{"title":"Azure","type":0,"sectionRef":"#","url":"/terraform-databricks-sra/docs/usage/Azure/","content":"","keywords":"","version":"Next"},{"title":"Overview‚Äã","type":1,"pageTitle":"Azure","url":"/terraform-databricks-sra/docs/usage/Azure/#overview","content":" Note For feedback, questions, and comments ‚Äî please open a GitHub Issue. Please review the Project Support section for important information on support and service terms.  The Security Reference Architecture (SRA) with Terraform provides a prescriptive deployment pattern for Databricks on Azure, designed for highly secure and regulated environments. It captures Databricks security best practices in Terraform templates, allowing organizations to programmatically deploy workspaces and supporting infrastructure with hardened, opinionated defaults.  This architecture emphasizes:  Secure by Default ‚Äì Environments are provisioned with restrictive networking, managed identites, and encryption controls.Governance &amp; Compliance ‚Äì Aligns with industry standards and simplifies audits through consistent, automated configurations.Scalability ‚Äì Built on a hub-and-spoke model to isolate workloads, enforce least privilege, and simplify cross-environment connectivity.Point-in-Time Design ‚Äì Each release reflects security best practices at that time; new releases may not be drop-in replacements.  ","version":"Next","tagName":"h2"},{"title":"Architecture Diagram‚Äã","type":1,"pageTitle":"Azure","url":"/terraform-databricks-sra/docs/usage/Azure/#architecture-diagram","content":"   The Azure implementation of the Security Reference Architecture (SRA) includes:  Hub-and-Spoke model for network segmentation, workload isolation, and to simplify cross-environment connectivity.Core Azure components such as Vnet Injection, privatelink endpoints, managed identities, Azure Storage Accounts, and Azure Key Vault.Core Databricks components including Unity Catalog, network connectivty configurations, and network policies for serverless compute.  ","version":"Next","tagName":"h2"},{"title":"Next Steps‚Äã","type":1,"pageTitle":"Azure","url":"/terraform-databricks-sra/docs/usage/Azure/#next-steps","content":" Review the Read Before Deploying section for critical considerations.Choose your Deployment Mode based on your infrastructure (full SRA-managed, bring-your-own hub, or bring-your-own spoke network).Explore the Configuration Reference for detailed information on all available configuration options including security features, CMK, SAT, and network egress controls.Understand the SRA Components Breakdown to learn about the included Azure and Databricks resources.Follow the Getting Started guide to deploy using Terraform.Check the Troubleshooting guide if you encounter any issues during deployment. ","version":"Next","tagName":"h2"},{"title":"Getting Started","type":0,"sectionRef":"#","url":"/terraform-databricks-sra/docs/usage/GCP/gettingstarted/","content":"","keywords":"","version":"Next"},{"title":"SRA Installation and Deployment Steps‚Äã","type":1,"pageTitle":"Getting Started","url":"/terraform-databricks-sra/docs/usage/GCP/gettingstarted/#sra-installation-and-deployment-steps","content":" Follow the steps below to deploy the Security Reference Architecture (SRA) using Terraform:  Clone the SRA repository.Install Terraform.Navigate to the gcp -&gt; examples folder and choose the deployment model to be used (byo_gcp_workspace_deployment or simple_workspace_deployment) and open up the *.tfvars.example file. Fill in the required values for all of the variables and relevant feature flags that are required for the deployment.Rename the file to terraform.tfvars. From the terminal, ensure you are in the correct working directory for the tf folder.Run terraform init.Run terraform validate.Run terraform plan.Run terraform apply.  NOTE: When deploying the workspace module, you must set the DATABRICKS_GOOGLE_SERVICE_ACCOUNT environment variable to the Service Account email that will be used for authentication.  Example:  export DATABRICKS_GOOGLE_SERVICE_ACCOUNT=&lt;&lt;Your GCP Service Account Email&gt;&gt;   ","version":"Next","tagName":"h2"},{"title":"Critical Next Steps‚Äã","type":1,"pageTitle":"Getting Started","url":"/terraform-databricks-sra/docs/usage/GCP/gettingstarted/#critical-next-steps","content":" The following steps outline essential security configurations that should be implemented after the initial deployment to further harden and operationalize the Databricks environment.  Implement a Front-End Mitigation Strategy: IP Access Lists: IP Access Lists enhance security by providing control over which networks can connect to your GCP Databricks account and workspaces.Front-End PrivateLink: Front-End Private Link establishes a private connection to the Databricks web application over the GCP backbone, preventing exposure to the public internet.  ","version":"Next","tagName":"h2"},{"title":"Additional Security recommendations‚Äã","type":1,"pageTitle":"Getting Started","url":"/terraform-databricks-sra/docs/usage/GCP/gettingstarted/#additional-security-recommendations","content":" Segment Workspaces for Data Separation: Use distinct workspaces for different teams or functions (e.g., security, marketing) to enforce data access boundaries and reduce risk exposure.Avoid Storing Production Datasets in Databricks File Store (DBFS): The DBFS root is accessible to all users in a workspace. Use external storage locations for production data and databases to ensure proper access control and auditing.Back Up Assets from the Databricks Control Plane: Regularly export and back up notebooks, jobs, and configurations using tools such as the Databricks Terraform Exporter.Regularly Restart Classic Compute Clusters: Restart clusters periodically to ensure the latest compute images and security patches are applied. Databricks recommends that admins restart clusters manually during a scheduled maintenance window to minimze the risk of disrupting a scheduled job or workflows.Implement a Tagging Strategy: Cluster and pool tags enable organizations to monitor costs and accurately attribute Databricks usage to specific business units or teams. These tags propagate to detailed DBU usage reports, supporting cost analysis and internal chargeback processes.Integrate CI/CD and Code Management: Evaluate workflow needs for Git-based version control and CI/CD automation. Incorporate code scanning, permission enforcement, and secret detection to enhance governance and operational efficiency.Run and Monitor the Security Analyis Tool (SAT): SAT analyzes your Databricks account and workspace configurations, providing recommendations to help you follow Databricks' security best practices. ","version":"Next","tagName":"h2"},{"title":"Deployment Modes Overview","type":0,"sectionRef":"#","url":"/terraform-databricks-sra/docs/usage/Azure/deploymentmodes/","content":"","keywords":"","version":"Next"},{"title":"Mode Comparison‚Äã","type":1,"pageTitle":"Deployment Modes Overview","url":"/terraform-databricks-sra/docs/usage/Azure/deploymentmodes/#mode-comparison","content":" Mode\tHub\tSpoke Network\tUse CaseMode 1: Full SRA\tSRA creates\tSRA creates\tGreenfield deployments Mode 2: Bring-your-own hub\tYou provide\tSRA creates\tExisting hub, new spoke Mode 3: Bring-your-own hub + spoke\tYou provide\tYou provide\tExisting infrastructure (most restrictive) Spoke Resource Group In both bring-your-own hub and bring-your-own hub + spoke options, you may optionally use SRA to create the spoke resource group by changing the value of the create_workspace_resource_group variable.  ","version":"Next","tagName":"h2"},{"title":"Decision Tree‚Äã","type":1,"pageTitle":"Deployment Modes Overview","url":"/terraform-databricks-sra/docs/usage/Azure/deploymentmodes/#decision-tree","content":" Use this decision tree to determine which deployment mode is right for you:    ","version":"Next","tagName":"h2"},{"title":"Quick Summary‚Äã","type":1,"pageTitle":"Deployment Modes Overview","url":"/terraform-databricks-sra/docs/usage/Azure/deploymentmodes/#quick-summary","content":" ","version":"Next","tagName":"h2"},{"title":"Mode 1: Full SRA (Default)‚Äã","type":1,"pageTitle":"Deployment Modes Overview","url":"/terraform-databricks-sra/docs/usage/Azure/deploymentmodes/#mode-1-full-sra-default","content":" Best for: Greenfield deploymentsSRA creates: Everything (hub, spoke, workspace)Template: template.example.tfvars  ","version":"Next","tagName":"h3"},{"title":"Mode 2: Bring-your-own Hub‚Äã","type":1,"pageTitle":"Deployment Modes Overview","url":"/terraform-databricks-sra/docs/usage/Azure/deploymentmodes/#mode-2-bring-your-own-hub","content":" Best for: Teams with existing hub infrastructureSRA creates: Spoke resources onlyYou provide: Hub VNET, Key Vault, metastore, NCC, network policyTemplate: template_byo_hub.example.tfvars  ","version":"Next","tagName":"h3"},{"title":"Mode 3: Bring-your-own Hub + Spoke‚Äã","type":1,"pageTitle":"Deployment Modes Overview","url":"/terraform-databricks-sra/docs/usage/Azure/deploymentmodes/#mode-3-bring-your-own-hub--spoke","content":" Best for: Organizations with fully existing infrastructureSRA creates: Workspace and related resources onlyYou provide: Hub VNET, Key Vault, metastore, NCC, network policy, spoke VNETTemplate: template_byo_spoke_network.example.tfvars  ","version":"Next","tagName":"h3"},{"title":"Next Steps‚Äã","type":1,"pageTitle":"Deployment Modes Overview","url":"/terraform-databricks-sra/docs/usage/Azure/deploymentmodes/#next-steps","content":" Select your deployment mode using the decision tree aboveClick on the mode link to view detailed configuration requirementsCopy the appropriate template file to terraform.tfvarsFill in required variables based on your modeReview Configuration Reference for optional featuresFollow Getting Started deployment stepsReview Troubleshooting if you encounter issues ","version":"Next","tagName":"h2"},{"title":"Mode 1: Full SRA (Default)","type":0,"sectionRef":"#","url":"/terraform-databricks-sra/docs/usage/Azure/mode1-full-sra/","content":"","keywords":"","version":"Next"},{"title":"Description‚Äã","type":1,"pageTitle":"Mode 1: Full SRA (Default)","url":"/terraform-databricks-sra/docs/usage/Azure/mode1-full-sra/#description","content":" SRA creates and manages all infrastructure including the hub VNET, Key Vault, firewall, and spoke workspace network.  ","version":"Next","tagName":"h2"},{"title":"What Gets Created‚Äã","type":1,"pageTitle":"Mode 1: Full SRA (Default)","url":"/terraform-databricks-sra/docs/usage/Azure/mode1-full-sra/#what-gets-created","content":" Resource\tComponent\tCreated by SRAHub Resource Group\tHub VNET + Azure Firewall\t‚úì Webauth Workspace\t‚úì CMK KeyVault\t‚úì Route Table\t‚úì Spoke Resource Group\tWorkspace\t‚úì Spoke VNET\t‚úì Back-end Private Endpoint\t‚úì UC Storage Account\t‚úì Account Console\tNCC (Network Connectivity Config)\t‚úì Network Policy\t‚úì Metastore\t‚úì  ","version":"Next","tagName":"h2"},{"title":"Configuration‚Äã","type":1,"pageTitle":"Mode 1: Full SRA (Default)","url":"/terraform-databricks-sra/docs/usage/Azure/mode1-full-sra/#configuration","content":" create_hub = true # Default create_workspace_vnet = true # Default   ","version":"Next","tagName":"h2"},{"title":"Required Variables‚Äã","type":1,"pageTitle":"Mode 1: Full SRA (Default)","url":"/terraform-databricks-sra/docs/usage/Azure/mode1-full-sra/#required-variables","content":" databricks_account_id = &quot;00000000-0000-0000-0000-000000000000&quot; location = &quot;westus2&quot; subscription_id = &quot;ffffffff-ffff-ffff-ffff-ffffffffffff&quot; resource_suffix = &quot;spoke&quot; hub_resource_suffix = &quot;srahub&quot; hub_vnet_cidr = &quot;10.0.0.0/22&quot; workspace_vnet = { cidr = &quot;10.0.4.0/22&quot; } tags = { Owner = &quot;user@example.com&quot; }   ","version":"Next","tagName":"h2"},{"title":"Required Variable Summary‚Äã","type":1,"pageTitle":"Mode 1: Full SRA (Default)","url":"/terraform-databricks-sra/docs/usage/Azure/mode1-full-sra/#required-variable-summary","content":" databricks_account_idlocationsubscription_idresource_suffixhub_resource_suffixhub_vnet_cidrworkspace_vnet.cidr  ","version":"Next","tagName":"h3"},{"title":"Template Variables File‚Äã","type":1,"pageTitle":"Mode 1: Full SRA (Default)","url":"/terraform-databricks-sra/docs/usage/Azure/mode1-full-sra/#template-variables-file","content":" Use template.example.tfvars as the template for this type of deployment.  ","version":"Next","tagName":"h2"},{"title":"Next Steps‚Äã","type":1,"pageTitle":"Mode 1: Full SRA (Default)","url":"/terraform-databricks-sra/docs/usage/Azure/mode1-full-sra/#next-steps","content":" Copy template.example.tfvars to terraform.tfvarsFill in required variablesReview Configuration Reference for optional featuresFollow Getting Started deployment steps ","version":"Next","tagName":"h2"},{"title":"Mode 3: Bring-your-own Hub and Spoke Network","type":0,"sectionRef":"#","url":"/terraform-databricks-sra/docs/usage/Azure/mode3-byo-hub-spoke/","content":"","keywords":"","version":"Next"},{"title":"Description‚Äã","type":1,"pageTitle":"Mode 3: Bring-your-own Hub and Spoke Network","url":"/terraform-databricks-sra/docs/usage/Azure/mode3-byo-hub-spoke/#description","content":" You provide both existing hub and spoke infrastructure. SRA is only responsible for creating the spoke workspace and related resources.  ","version":"Next","tagName":"h2"},{"title":"What Gets Created‚Äã","type":1,"pageTitle":"Mode 3: Bring-your-own Hub and Spoke Network","url":"/terraform-databricks-sra/docs/usage/Azure/mode3-byo-hub-spoke/#what-gets-created","content":" Resource\tComponent\tCreated by SRAHub Resource Group\tHub VNET + Azure Firewall\t‚úó (You provide) Webauth Workspace\t‚úó (You provide) CMK KeyVault\t‚úó (You provide) Route Table\t‚úó (You provide) Spoke Resource Group\tWorkspace\t‚úì Spoke VNET\t‚úó (You provide) Back-end Private Endpoint\t‚úì UC Storage Account\t‚úì Account Console\tNCC (Network Connectivity Config)\t‚úó (You provide) Network Policy\t‚úó (You provide) Metastore\t‚úó (You provide)  ","version":"Next","tagName":"h2"},{"title":"Configuration‚Äã","type":1,"pageTitle":"Mode 3: Bring-your-own Hub and Spoke Network","url":"/terraform-databricks-sra/docs/usage/Azure/mode3-byo-hub-spoke/#configuration","content":" create_hub = false # You provide hub create_workspace_vnet = false # You provide spoke network   ","version":"Next","tagName":"h2"},{"title":"Required Variables‚Äã","type":1,"pageTitle":"Mode 3: Bring-your-own Hub and Spoke Network","url":"/terraform-databricks-sra/docs/usage/Azure/mode3-byo-hub-spoke/#required-variables","content":" Additional Requirements When bringing your own hub and spoke, you must provide all hub and spoke infrastructure details. This combines all requirements from Mode 2 with spoke network details.  # Disable hub and spoke creation create_hub = false create_workspace_vnet = false # Basic configuration databricks_account_id = &quot;00000000-0000-0000-0000-000000000000&quot; location = &quot;westus2&quot; subscription_id = &quot;ffffffff-ffff-ffff-ffff-ffffffffffff&quot; resource_suffix = &quot;spoke&quot; # REQUIRED: Existing metastore from your hub databricks_metastore_id = &quot;00000000-0000-0000-0000-000000000000&quot; # REQUIRED: Existing hub VNET details (for spoke network peering) existing_hub_vnet = { route_table_id = &quot;/subscriptions/.../providers/Microsoft.Network/routeTables/rt-hub&quot; vnet_id = &quot;/subscriptions/.../providers/Microsoft.Network/virtualNetworks/vnet-hub&quot; } # REQUIRED: Existing NCC and network policy existing_ncc_id = &quot;your-ncc-id&quot; existing_network_policy_id = &quot;your-network-policy-id&quot; # REQUIRED if cmk_enabled = true (default) existing_cmk_ids = { key_vault_id = &quot;/subscriptions/.../providers/Microsoft.KeyVault/vaults/kv-hub&quot; managed_disk_key_id = &quot;https://kv-hub.vault.azure.net/keys/cmk-disk/version&quot; managed_services_key_id = &quot;https://kv-hub.vault.azure.net/keys/cmk-services/version&quot; } # REQUIRED: BYO workspace network configuration existing_workspace_vnet = { network_configuration = { virtual_network_id = &quot;/subscriptions/.../virtualNetworks/vnet-spoke&quot; private_subnet_id = &quot;/subscriptions/.../subnets/container&quot; public_subnet_id = &quot;/subscriptions/.../subnets/host&quot; private_endpoint_subnet_id = &quot;/subscriptions/.../subnets/private-endpoints&quot; private_subnet_network_security_group_association_id = &quot;/subscriptions/.../subnets/container&quot; public_subnet_network_security_group_association_id = &quot;/subscriptions/.../subnets/host&quot; } dns_zone_ids = { backend = &quot;/subscriptions/.../privateDnsZones/privatelink.azuredatabricks.net&quot; dfs = &quot;/subscriptions/.../privateDnsZones/privatelink.dfs.core.windows.net&quot; blob = &quot;/subscriptions/.../privateDnsZones/privatelink.blob.core.windows.net&quot; } } # Network egress allowed_fqdns = [] hub_allowed_urls = [] tags = { Owner = &quot;user@example.com&quot; }   ","version":"Next","tagName":"h2"},{"title":"Required Variable Summary‚Äã","type":1,"pageTitle":"Mode 3: Bring-your-own Hub and Spoke Network","url":"/terraform-databricks-sra/docs/usage/Azure/mode3-byo-hub-spoke/#required-variable-summary","content":" databricks_account_idlocationsubscription_idresource_suffixdatabricks_metastore_idexisting_ncc_idexisting_network_policy_idexisting_hub_vnetexisting_cmk_ids (if cmk_enabled = true)existing_workspace_vnet  ","version":"Next","tagName":"h3"},{"title":"Template File‚Äã","type":1,"pageTitle":"Mode 3: Bring-your-own Hub and Spoke Network","url":"/terraform-databricks-sra/docs/usage/Azure/mode3-byo-hub-spoke/#template-file","content":" Use template_byo_spoke_network.example.tfvars as your starting point.  ","version":"Next","tagName":"h2"},{"title":"Validation Rules‚Äã","type":1,"pageTitle":"Mode 3: Bring-your-own Hub and Spoke Network","url":"/terraform-databricks-sra/docs/usage/Azure/mode3-byo-hub-spoke/#validation-rules","content":" When create_hub = false:  Must provide databricks_metastore_idMust provide existing_ncc_idMust provide existing_network_policy_idMust provide existing_hub_vnetMust provide existing_cmk_ids if cmk_enabled = trueMust NOT provide existing_cmk_ids if cmk_enabled = false  When create_workspace_vnet = false:  Must provide existing_workspace_vnetMust NOT provide workspace_vnet  ","version":"Next","tagName":"h2"},{"title":"Next Steps‚Äã","type":1,"pageTitle":"Mode 3: Bring-your-own Hub and Spoke Network","url":"/terraform-databricks-sra/docs/usage/Azure/mode3-byo-hub-spoke/#next-steps","content":" Copy template_byo_spoke_network.example.tfvars to terraform.tfvarsFill in required variables from your existing infrastructureReview Configuration Reference for optional featuresFollow Getting Started deployment steps ","version":"Next","tagName":"h2"},{"title":"GCP","type":0,"sectionRef":"#","url":"/terraform-databricks-sra/docs/usage/GCP/","content":"","keywords":"","version":"Next"},{"title":"Overview‚Äã","type":1,"pageTitle":"GCP","url":"/terraform-databricks-sra/docs/usage/GCP/#overview","content":" Note For feedback, questions, and comments ‚Äî please open a GitHub Issue. Please review the Project Support section for important information on support and service terms.  The Security Reference Architecture (SRA) with Terraform provides a prescriptive deployment pattern for Databricks on GCP, designed for highly secure and regulated environments. It captures Databricks security best practices in Terraform templates, allowing organizations to programmatically deploy workspaces and supporting infrastructure with hardened, opinionated defaults.  This architecture emphasizes:  Secure by Default ‚Äì Deployments leverage customer-managed VPCs, Private Service Connect (PSC), and customer-managed encryption keys (CMEK) to ensure all data plane and control plane communication remains within Google‚Äôs private network and under customer control.Governance &amp; Compliance ‚Äì esigned to meet stringent enterprise and regulatory requirements by standardizing secure network boundaries, key management, and private connectivity through reusable Terraform modules.Scalability ‚Äì Uses a modular network foundation that supports multiple workspaces across shared VPCs, enabling consistent security enforcement, resource isolation, and simplified lifecycle management.Point-in-Time Design ‚Äì Each release reflects security best practices at that time; new releases may not be drop-in replacements.  ","version":"Next","tagName":"h2"},{"title":"Architecture Diagram‚Äã","type":1,"pageTitle":"GCP","url":"/terraform-databricks-sra/docs/usage/GCP/#architecture-diagram","content":"   The GCP implementation of the Security Reference Architecture (SRA) includes:  A customer managed VPC model for centralized network management, workload isolation, and simplified multi-workspace connectivity.Core GCP components such as customer-managed VPCs, Private Service Connect (PSC) for back-end connectivity, Cloud KMS for customer-managed encryption keys (CMEK), and Private Cloud DNS for internal name resolution.  ","version":"Next","tagName":"h2"},{"title":"Next Steps‚Äã","type":1,"pageTitle":"GCP","url":"/terraform-databricks-sra/docs/usage/GCP/#next-steps","content":" Review the Read Before Deploying section for critical considerations.Explore the SRA Components Breakdown to understand the included GCP and Databricks resources.Follow the Getting Started guide to deploy using Terraform. ","version":"Next","tagName":"h2"},{"title":"Configuration Reference","type":0,"sectionRef":"#","url":"/terraform-databricks-sra/docs/usage/Azure/configuration/","content":"","keywords":"","version":"Next"},{"title":"Core Configuration‚Äã","type":1,"pageTitle":"Configuration Reference","url":"/terraform-databricks-sra/docs/usage/Azure/configuration/#core-configuration","content":" ","version":"Next","tagName":"h2"},{"title":"Required Variables‚Äã","type":1,"pageTitle":"Configuration Reference","url":"/terraform-databricks-sra/docs/usage/Azure/configuration/#required-variables","content":" Variable\tType\tDescriptiondatabricks_account_id\tstring\tDatabricks account ID for account-level operations location\tstring\tAzure region (e.g., &quot;westus&quot;, &quot;eastus2&quot;) subscription_id\tstring\tAzure subscription ID to deploy into resource_suffix\tstring\tSuffix for naming workspace resources (e.g., &quot;spoke&quot;, &quot;prod&quot;)  Example:  databricks_account_id = &quot;00000000-0000-0000-0000-000000000000&quot; location = &quot;westus2&quot; subscription_id = &quot;ffffffff-ffff-ffff-ffff-ffffffffffff&quot; resource_suffix = &quot;spoke&quot;   ","version":"Next","tagName":"h3"},{"title":"Hub Configuration‚Äã","type":1,"pageTitle":"Configuration Reference","url":"/terraform-databricks-sra/docs/usage/Azure/configuration/#hub-configuration","content":" Variable\tType\tDefault\tRequired When\tDescriptioncreate_hub\tbool\ttrue\tAlways\tWhether to create hub infrastructure hub_resource_suffix\tstring\t&quot;&quot;\tcreate_hub = true\tHub resource naming suffix hub_vnet_cidr\tstring\t&quot;&quot;\tcreate_hub = true\tHub VNET CIDR block  Example:  create_hub = true hub_resource_suffix = &quot;srahub&quot; hub_vnet_cidr = &quot;10.0.0.0/22&quot;   ","version":"Next","tagName":"h3"},{"title":"Workspace (Spoke) Network Configuration‚Äã","type":1,"pageTitle":"Configuration Reference","url":"/terraform-databricks-sra/docs/usage/Azure/configuration/#workspace-spoke-network-configuration","content":" Variable\tType\tDefault\tRequired When\tDescriptioncreate_workspace_vnet\tbool\ttrue\tAlways\tWhether to create spoke VNET workspace_vnet\tobject\tnull\tcreate_workspace_vnet = true\tSpoke network configuration existing_workspace_vnet\tobject\tnull\tcreate_workspace_vnet = false\tExisting network details  Example (SRA-managed spoke network):  create_workspace_vnet = true workspace_vnet = { cidr = &quot;10.0.4.0/22&quot; new_bits = 4 # Optional: for subnet sizing }   Example (Bring-your-own hub and spoke network):  create_workspace_vnet = false existing_workspace_vnet = { network_configuration = { virtual_network_id = &quot;/subscriptions/.../virtualNetworks/vnet-spoke&quot; private_subnet_id = &quot;/subscriptions/.../subnets/container&quot; public_subnet_id = &quot;/subscriptions/.../subnets/host&quot; private_endpoint_subnet_id = &quot;/subscriptions/.../subnets/private-endpoints&quot; private_subnet_network_security_group_association_id = &quot;/subscriptions/.../subnets/container&quot; public_subnet_network_security_group_association_id = &quot;/subscriptions/.../subnets/host&quot; } dns_zone_ids = { backend = &quot;/subscriptions/.../privateDnsZones/privatelink.azuredatabricks.net&quot; dfs = &quot;/subscriptions/.../privateDnsZones/privatelink.dfs.core.windows.net&quot; blob = &quot;/subscriptions/.../privateDnsZones/privatelink.blob.core.windows.net&quot; } }   ","version":"Next","tagName":"h3"},{"title":"Tags‚Äã","type":1,"pageTitle":"Configuration Reference","url":"/terraform-databricks-sra/docs/usage/Azure/configuration/#tags","content":" Variable\tType\tDefault\tDescriptiontags\tmap(string)\t{}\tTags to apply to all resources  Example:  tags = { Owner = &quot;user@example.com&quot; Environment = &quot;production&quot; CostCenter = &quot;engineering&quot; }     ","version":"Next","tagName":"h3"},{"title":"Network Egress‚Äã","type":1,"pageTitle":"Configuration Reference","url":"/terraform-databricks-sra/docs/usage/Azure/configuration/#network-egress","content":" Control what internet resources your workspaces can access.  Default: Zero Internet Access By default, SRA implements a zero-trust network egress policy where all workspaces have no internet access. You must explicitly allow access to specific FQDNs.  ","version":"Next","tagName":"h2"},{"title":"allowed_fqdns‚Äã","type":1,"pageTitle":"Configuration Reference","url":"/terraform-databricks-sra/docs/usage/Azure/configuration/#allowed_fqdns","content":" Type: list(string)Default: [] (no internet access)Applies to: Spoke workspace (classic and serverless compute)  List of FQDNs that spoke workspace can access. Supports wildcards (e.g., *.pypi.org).  Example - Python Packages:  allowed_fqdns = [ &quot;python.org&quot;, &quot;*.python.org&quot;, &quot;pypi.org&quot;, &quot;*.pypi.org&quot;, &quot;pythonhosted.org&quot;, &quot;*.pythonhosted.org&quot; ]   Example - R Packages:  allowed_fqdns = [ &quot;cran.r-project.org&quot;, &quot;*.cran.r-project.org&quot;, &quot;r-project.org&quot; ]   ","version":"Next","tagName":"h3"},{"title":"hub_allowed_urls‚Äã","type":1,"pageTitle":"Configuration Reference","url":"/terraform-databricks-sra/docs/usage/Azure/configuration/#hub_allowed_urls","content":" Type: set(string)Default: [] (no internet access)Applies to: Hub workspace serverless compute only  List of URLs for hub workspace serverless. No wildcard support.  Example:  hub_allowed_urls = [ &quot;management.azure.com&quot;, &quot;login.microsoftonline.com&quot;, &quot;python.org&quot;, # No wildcard &quot;pypi.org&quot;, &quot;pythonhosted.org&quot; ]   ","version":"Next","tagName":"h3"},{"title":"Key Differences‚Äã","type":1,"pageTitle":"Configuration Reference","url":"/terraform-databricks-sra/docs/usage/Azure/configuration/#key-differences","content":" Feature\tallowed_fqdns\thub_allowed_urlsScope\tSpoke workspace and hub workspace classic\tHub workspace serverless Classic Compute\tYes (hub and spoke)\tNo Serverless Compute\tYes (spoke only)\tYes (hub only)  ","version":"Next","tagName":"h3"},{"title":"SAT URL Requirements‚Äã","type":1,"pageTitle":"Configuration Reference","url":"/terraform-databricks-sra/docs/usage/Azure/configuration/#sat-url-requirements","content":" If you enable SAT, you must include specific URLs based on the compute type:  For SAT on Classic Compute:  sat_configuration = { enabled = true run_on_serverless = false # Default } allowed_fqdns = [ &quot;management.azure.com&quot;, &quot;login.microsoftonline.com&quot;, &quot;python.org&quot;, &quot;*.python.org&quot;, &quot;pypi.org&quot;, &quot;*.pypi.org&quot;, &quot;pythonhosted.org&quot;, &quot;*.pythonhosted.org&quot; ]   For SAT on Serverless:  sat_configuration = { enabled = true run_on_serverless = true } hub_allowed_urls = [ &quot;management.azure.com&quot;, &quot;login.microsoftonline.com&quot;, &quot;python.org&quot;, &quot;pypi.org&quot;, &quot;pythonhosted.org&quot; ]   Validation Terraform will validate that SAT-required URLs are present if SAT is enabled.    ","version":"Next","tagName":"h3"},{"title":"Security & Compliance‚Äã","type":1,"pageTitle":"Configuration Reference","url":"/terraform-databricks-sra/docs/usage/Azure/configuration/#security--compliance","content":" The SRA supports advanced security and compliance features through the workspace_security_compliance configuration.  ","version":"Next","tagName":"h2"},{"title":"workspace_security_compliance‚Äã","type":1,"pageTitle":"Configuration Reference","url":"/terraform-databricks-sra/docs/usage/Azure/configuration/#workspace_security_compliance","content":" Type: objectDefault: null (all features disabled)  Configures Enhanced Security Compliance (ESC) features.  Available Fields:  Field\tType\tDefault\tDescriptioncompliance_security_profile_enabled\tbool\tnull\tEnable Compliance Security Profile (CSP) compliance_security_profile_standards\tlist(string)\t[]\tCompliance standards (e.g., [&quot;HIPAA&quot;]) enhanced_security_monitoring_enabled\tbool\tnull\tEnable Enhanced Security Monitoring (ESM) automatic_cluster_update_enabled\tbool\tnull\tEnable automatic cluster updates  ","version":"Next","tagName":"h3"},{"title":"Compliance Security Profile (CSP)‚Äã","type":1,"pageTitle":"Configuration Reference","url":"/terraform-databricks-sra/docs/usage/Azure/configuration/#compliance-security-profile-csp","content":" Enables Databricks Compliance Security Profile.  Example - HIPAA Compliance:  workspace_security_compliance = { compliance_security_profile_enabled = true compliance_security_profile_standards = [&quot;HIPAA&quot;] }   Available Standards:  Review Databricks documentation for all available standards.  Validation Rule If you specify compliance_security_profile_standards, you must set compliance_security_profile_enabled = true.  ","version":"Next","tagName":"h3"},{"title":"Enhanced Security Monitoring (ESM)‚Äã","type":1,"pageTitle":"Configuration Reference","url":"/terraform-databricks-sra/docs/usage/Azure/configuration/#enhanced-security-monitoring-esm","content":" Enables enhanced security monitoring.  Example:  workspace_security_compliance = { enhanced_security_monitoring_enabled = true }   ","version":"Next","tagName":"h3"},{"title":"Complete Example‚Äã","type":1,"pageTitle":"Configuration Reference","url":"/terraform-databricks-sra/docs/usage/Azure/configuration/#complete-example","content":" Enable all security and compliance features:  workspace_security_compliance = { compliance_security_profile_enabled = true compliance_security_profile_standards = [&quot;HIPAA&quot;] enhanced_security_monitoring_enabled = true automatic_cluster_update_enabled = true }   ","version":"Next","tagName":"h3"},{"title":"Customer-Managed Keys (CMK)‚Äã","type":1,"pageTitle":"Configuration Reference","url":"/terraform-databricks-sra/docs/usage/Azure/configuration/#customer-managed-keys-cmk","content":" Controls enabling customer-managed keys features.  ","version":"Next","tagName":"h2"},{"title":"cmk_enabled‚Äã","type":1,"pageTitle":"Configuration Reference","url":"/terraform-databricks-sra/docs/usage/Azure/configuration/#cmk_enabled","content":" Type: boolDefault: true  Whether to enable customer-managed keys for workspace encryption.  When enabled (default):  CMK is enabled wherever possibleKeys are managed in an Azure Key Vault in the hub environment (created by SRA if create_hub = true)  To disable CMK:  cmk_enabled = false   ","version":"Next","tagName":"h3"},{"title":"existing_cmk_ids‚Äã","type":1,"pageTitle":"Configuration Reference","url":"/terraform-databricks-sra/docs/usage/Azure/configuration/#existing_cmk_ids","content":" Type: objectDefault: nullRequired when: create_hub = false AND cmk_enabled = true  Provide existing CMK key IDs when using a bring-your-own hub deployment with CMK enabled.  Structure:  existing_cmk_ids = { key_vault_id = &quot;/subscriptions/.../Microsoft.KeyVault/vaults/kv-hub&quot; managed_disk_key_id = &quot;https://kv-hub.vault.azure.net/keys/cmk-disk/abc123&quot; managed_services_key_id = &quot;https://kv-hub.vault.azure.net/keys/cmk-services/def456&quot; }   Fields:  key_vault_id - Azure Resource ID of the Key Vault containing the keysmanaged_disk_key_id - Full key URL for encrypting managed disksmanaged_services_key_id - Full key URL for encrypting managed services  Validation Rules If create_hub = false and cmk_enabled = true, you must provide existing_cmk_idsIf create_hub = true, you must not provide existing_cmk_ids (SRA creates keys)  ","version":"Next","tagName":"h3"},{"title":"Example Configurations‚Äã","type":1,"pageTitle":"Configuration Reference","url":"/terraform-databricks-sra/docs/usage/Azure/configuration/#example-configurations","content":" Mode 1 (Full SRA-managed with CMK):  create_hub = true cmk_enabled = true # Default # No existing_cmk_ids needed - SRA creates keys   Mode 1 (Full SRA-managed without CMK):  create_hub = true cmk_enabled = false # No CMK resources created   Mode 2 (BYO Hub with CMK):  create_hub = false cmk_enabled = true existing_cmk_ids = { key_vault_id = &quot;/subscriptions/.../vaults/kv-hub&quot; managed_disk_key_id = &quot;https://kv-hub.vault.azure.net/keys/...&quot; managed_services_key_id = &quot;https://kv-hub.vault.azure.net/keys/...&quot; }   Mode 2 (BYO Hub without CMK):  create_hub = false cmk_enabled = false # No existing_cmk_ids needed     ","version":"Next","tagName":"h3"},{"title":"Security Analysis Tool (SAT)‚Äã","type":1,"pageTitle":"Configuration Reference","url":"/terraform-databricks-sra/docs/usage/Azure/configuration/#security-analysis-tool-sat","content":" Configure the Security Analysis Tool for continuous security monitoring.  Disabled by Default SAT is disabled by default to support highly restricted no-egress deployments. Enable by setting sat_configuration.enabled = true.  ","version":"Next","tagName":"h2"},{"title":"sat_configuration‚Äã","type":1,"pageTitle":"Configuration Reference","url":"/terraform-databricks-sra/docs/usage/Azure/configuration/#sat_configuration","content":" Type: objectDefault: { enabled = false }  Field\tType\tDefault\tDescriptionenabled\tbool\tfalse\tEnable SAT deployment run_on_serverless\tbool\tfalse\tRun on serverless (vs classic compute) schema_name\tstring\t&quot;sat&quot;\tSchema name for SAT tables catalog_name\tstring\t&quot;sat&quot;\tCatalog name for SAT resources proxies\tmap(any)\t{}\tHTTP Proxy configuration for SAT operations  ","version":"Next","tagName":"h3"},{"title":"Enable SAT on Classic Compute‚Äã","type":1,"pageTitle":"Configuration Reference","url":"/terraform-databricks-sra/docs/usage/Azure/configuration/#enable-sat-on-classic-compute","content":" Configuration:  sat_configuration = { enabled = true run_on_serverless = false # Default } # REQUIRED: Network egress for classic compute SAT allowed_fqdns = [ &quot;management.azure.com&quot;, &quot;login.microsoftonline.com&quot;, &quot;python.org&quot;, &quot;*.python.org&quot;, &quot;pypi.org&quot;, &quot;*.pypi.org&quot;, &quot;pythonhosted.org&quot;, &quot;*.pythonhosted.org&quot; ]   Behavior:  Deployed in hub (WEBAUTH) workspace by defaultCan inspect all workspaces in the subscriptionRequires classic compute cluster  ","version":"Next","tagName":"h3"},{"title":"Enable SAT on Serverless‚Äã","type":1,"pageTitle":"Configuration Reference","url":"/terraform-databricks-sra/docs/usage/Azure/configuration/#enable-sat-on-serverless","content":" Configuration:  sat_configuration = { enabled = true run_on_serverless = true } # REQUIRED: Network egress for serverless SAT hub_allowed_urls = [ &quot;management.azure.com&quot;, &quot;login.microsoftonline.com&quot;, &quot;python.org&quot;, &quot;pypi.org&quot;, &quot;pythonhosted.org&quot; ]   Serverless Limitation When running SAT on serverless, it can only inspect the workspace it's deployed in. Use classic compute to inspect multiple workspaces across the subscription.  ","version":"Next","tagName":"h3"},{"title":"Customize SAT Catalog and Schema‚Äã","type":1,"pageTitle":"Configuration Reference","url":"/terraform-databricks-sra/docs/usage/Azure/configuration/#customize-sat-catalog-and-schema","content":" Configuration:  sat_configuration = { enabled = true catalog_name = &quot;security_monitoring&quot; schema_name = &quot;analysis_results&quot; }   ","version":"Next","tagName":"h3"},{"title":"sat_service_principal‚Äã","type":1,"pageTitle":"Configuration Reference","url":"/terraform-databricks-sra/docs/usage/Azure/configuration/#sat_service_principal","content":" Type: objectDefault: { name = &quot;spSAT&quot; }  Configure or provide an existing service principal for SAT operations.  Service Principal Requirements Some users may not have permissions to create Entra ID service principals. In this case, provide an existing service principal credentials.  Option 1: Let SRA create the service principal (default)  # Uses default name &quot;spSAT&quot; sat_service_principal = {} # Or customize the name sat_service_principal = { name = &quot;spSATDev&quot; }   Option 2: Use existing service principal  sat_service_principal = { client_id = &quot;00000000-0000-0000-0000-000000000000&quot; client_secret = &quot;your-service-principal-secret&quot; }   Important When providing client_id, you must also provide client_secret. Both fields are required together.  ","version":"Next","tagName":"h3"},{"title":"SAT Availability‚Äã","type":1,"pageTitle":"Configuration Reference","url":"/terraform-databricks-sra/docs/usage/Azure/configuration/#sat-availability","content":" SAT can only be deployed when create_hub = true. If using Mode 2 (BYO Hub), SAT must be deployed separately in your existing hub workspace.    ","version":"Next","tagName":"h3"},{"title":"Resource Naming‚Äã","type":1,"pageTitle":"Configuration Reference","url":"/terraform-databricks-sra/docs/usage/Azure/configuration/#resource-naming","content":" Customize names of specific Azure resources to meet your organization's naming conventions.  ","version":"Next","tagName":"h2"},{"title":"workspace_name_overrides‚Äã","type":1,"pageTitle":"Configuration Reference","url":"/terraform-databricks-sra/docs/usage/Azure/configuration/#workspace_name_overrides","content":" Type: map(string)Default: {}  Override default names for workspace and related resources. Keys should match the resource types you want to customize.  Available Override Keys:  Key\tResource\tAzure Naming Constraintsdatabricks_workspace\tDatabricks workspace\t3-64 chars, alphanumeric and hyphens private_endpoint\tPrivate endpoints (base name)\t1-80 chars, alphanumeric, hyphens, underscores storage_account\tStorage account\t3-24 chars, lowercase letters and numbers only resource_group\tResource group\t1-90 chars, alphanumeric, hyphens, underscores, periods  Example - Custom Workspace Name:  workspace_name_overrides = { databricks_workspace = &quot;dbx-prod-analytics&quot; }   Example - Multiple Overrides:  workspace_name_overrides = { databricks_workspace = &quot;dbx-prod-workspace&quot; storage_account = &quot;sadbxprod001&quot; resource_group = &quot;rg-databricks-prod&quot; }   Naming Conventions When overriding names: Ensure names comply with Azure resource naming constraints (i.e. Storage account names must be globally unique)Private endpoint names have suffixes added (-backend, -webauth)Consider your organization's naming policies and tagging strategy    ","version":"Next","tagName":"h3"},{"title":"Testing Variables‚Äã","type":1,"pageTitle":"Configuration Reference","url":"/terraform-databricks-sra/docs/usage/Azure/configuration/#testing-variables","content":" Testing Environments Only These variables are for testing and development environments only. Do not use in production as they can lead to accidental data loss.  ","version":"Next","tagName":"h2"},{"title":"catalog_force_destroy‚Äã","type":1,"pageTitle":"Configuration Reference","url":"/terraform-databricks-sra/docs/usage/Azure/configuration/#catalog_force_destroy","content":" Type: boolDefault: false  Allows Terraform to force destroy spoke workspace catalogs, bypassing Databricks' protection against accidental catalog deletion.  Configuration:  catalog_force_destroy = true # Testing only!   What it does:  Enables terraform destroy to delete catalogs containing dataSkips the manual confirmation normally requiredUseful for automated testing and rapid iteration  Production risk:Catalogs contain schemas, tables, views, and metadata. Enabling force destroy in production could lead to accidental data loss if terraform destroy is run unintentionally.  ","version":"Next","tagName":"h3"},{"title":"sat_force_destroy‚Äã","type":1,"pageTitle":"Configuration Reference","url":"/terraform-databricks-sra/docs/usage/Azure/configuration/#sat_force_destroy","content":" Type: boolDefault: false  Allows Terraform to force destroy the SAT catalog.  Configuration:  sat_force_destroy = true # Testing only!   Use case:In testing/development environments where you need to repeatedly create and destroy infrastructure, these flags enable faster iteration.  ","version":"Next","tagName":"h3"},{"title":"Example - Testing Environment‚Äã","type":1,"pageTitle":"Configuration Reference","url":"/terraform-databricks-sra/docs/usage/Azure/configuration/#example---testing-environment","content":" # terraform.tfvars for testing environment # Enable force destroy for testing environments catalog_force_destroy = true sat_force_destroy = true     ","version":"Next","tagName":"h3"},{"title":"Variable Dependencies‚Äã","type":1,"pageTitle":"Configuration Reference","url":"/terraform-databricks-sra/docs/usage/Azure/configuration/#variable-dependencies","content":" ","version":"Next","tagName":"h2"},{"title":"Cross-Variable Requirements‚Äã","type":1,"pageTitle":"Configuration Reference","url":"/terraform-databricks-sra/docs/usage/Azure/configuration/#cross-variable-requirements","content":" Understanding variable dependencies helps avoid configuration errors.  If you set...\tThen you must also set...\tWhycreate_hub = false\tdatabricks_metastore_id\tSpoke needs metastore reference create_hub = false\texisting_ncc_id\tSpoke needs NCC for serverless create_hub = false\texisting_network_policy_id\tSpoke needs network policy create_hub = false\texisting_hub_vnet\tSpoke needs hub VNET for peering create_hub = false AND cmk_enabled = true\texisting_cmk_ids\tSpoke needs CMK keys for encryption create_workspace_vnet = true\tworkspace_vnet\tSRA needs CIDR for spoke network create_workspace_vnet = false\texisting_workspace_vnet\tWorkspace needs existing network details sat_configuration.enabled = true AND run_on_serverless = false\tSAT URLs in allowed_fqdns\tClassic SAT needs internet access sat_configuration.enabled = true AND run_on_serverless = true\tSAT URLs in hub_allowed_urls\tServerless SAT needs internet access compliance_security_profile_standards = [...]\tcompliance_security_profile_enabled = true\tStandards require profile enabled  ","version":"Next","tagName":"h3"},{"title":"Mutual Exclusions‚Äã","type":1,"pageTitle":"Configuration Reference","url":"/terraform-databricks-sra/docs/usage/Azure/configuration/#mutual-exclusions","content":" Some variables cannot be used together:  Cannot use together\tReasonworkspace_vnet AND existing_workspace_vnet\tChoose one: SRA-managed or existing create_hub = true AND existing_cmk_ids\tSRA creates CMK when creating hub hub_vnet_cidr when create_hub = false\tNot creating hub, don't need hub CIDR    ","version":"Next","tagName":"h3"},{"title":"Next Steps‚Äã","type":1,"pageTitle":"Configuration Reference","url":"/terraform-databricks-sra/docs/usage/Azure/configuration/#next-steps","content":" Review Deployment Modes to choose your architectureSee Components for detailed feature documentationFollow Getting Started for deployment stepsCheck Troubleshooting if you encounter issues ","version":"Next","tagName":"h2"},{"title":"Troubleshooting","type":0,"sectionRef":"#","url":"/terraform-databricks-sra/docs/usage/Azure/troubleshooting/","content":"","keywords":"","version":"Next"},{"title":"Provider Authentication Issues‚Äã","type":1,"pageTitle":"Troubleshooting","url":"/terraform-databricks-sra/docs/usage/Azure/troubleshooting/#provider-authentication-issues","content":" ","version":"Next","tagName":"h2"},{"title":"Azure CLI Tenant ID Error‚Äã","type":1,"pageTitle":"Troubleshooting","url":"/terraform-databricks-sra/docs/usage/Azure/troubleshooting/#azure-cli-tenant-id-error","content":" Error:  Error: cannot create mws network connectivity config: io.jsonwebtoken.IncorrectClaimException: Expected iss claim to be: https://sts.windows.net/00000000-0000-0000-0000-000000000000/, but was: https://sts.windows.net/ffffffff-ffff-ffff-ffff-ffffffffffff/   Cause: Running Terraform in a tenant where you are a guest user, or with multiple Azure accounts configured.  Solution:  Set the Azure Tenant ID by exporting the ARM_TENANT_ID environment variable:  export ARM_TENANT_ID=&quot;00000000-0000-0000-0000-000000000000&quot;   Alternatively, set the tenant ID directly in the Databricks provider configuration:  provider &quot;databricks&quot; { azure_tenant_id = &quot;00000000-0000-0000-0000-000000000000&quot; # ... other config }     ","version":"Next","tagName":"h3"},{"title":"Workspace Access Issues‚Äã","type":1,"pageTitle":"Troubleshooting","url":"/terraform-databricks-sra/docs/usage/Azure/troubleshooting/#workspace-access-issues","content":" ","version":"Next","tagName":"h2"},{"title":"Cannot Read Current User Error‚Äã","type":1,"pageTitle":"Troubleshooting","url":"/terraform-databricks-sra/docs/usage/Azure/troubleshooting/#cannot-read-current-user-error","content":" Error:  Error: cannot read current user: Unauthorized access to Org: 0000000000000000 with module.sat[0].module.sat.data.databricks_current_user.me, on .terraform/modules/sat.sat/terraform/common/data.tf line 1, in data &quot;databricks_current_user&quot; &quot;me&quot;: 1: data &quot;databricks_current_user&quot; &quot;me&quot; {}   Cause: The user or service principal running Terraform does not have access to the newly created workspace yet.  Solution for User Identity:  Log in to the newly created workspace by clicking &quot;Launch Workspace&quot; in the Azure portalEnsure this is done as the same user running TerraformRe-run terraform apply  Solution for Service Principal:  The SRA automatically grants workspace admin permissions to the deploying service principal. This error should not occur with service principal authentication. If it does, verify:  The service principal is correctly configured in the Databricks providerThe service principal has sufficient permissions in the Azure subscriptionThe databricks_permission_assignment resources are being created   --- ## Validation Errors ### SAT URL Validation Error (Classic Compute) **Error:** ```bash Error: Since SAT is enabled and is not running on serverless, you must include SAT-required URLs in the allowed_fqdns variable.   Cause: SAT is enabled on classic compute but required URLs are missing from allowed_fqdns.  Solution:  Add the required URLs to allowed_fqdns:  sat_configuration = { enabled = true run_on_serverless = false # Default } allowed_fqdns = [ &quot;management.azure.com&quot;, &quot;login.microsoftonline.com&quot;, &quot;python.org&quot;, &quot;*.python.org&quot;, &quot;pypi.org&quot;, &quot;*.pypi.org&quot;, &quot;pythonhosted.org&quot;, &quot;*.pythonhosted.org&quot; ]   ","version":"Next","tagName":"h3"},{"title":"SAT URL Validation Error (Serverless)‚Äã","type":1,"pageTitle":"Troubleshooting","url":"/terraform-databricks-sra/docs/usage/Azure/troubleshooting/#sat-url-validation-error-serverless","content":" Error:  Error: Since SAT is enabled and running on serverless you must include SAT-required URLs in the hub_allowed_urls variable.   Cause: SAT is enabled on serverless but required URLs are missing from hub_allowed_urls.  Solution:  Add the required URLs to hub_allowed_urls (note: no wildcards):  sat_configuration = { enabled = true run_on_serverless = true } hub_allowed_urls = [ &quot;management.azure.com&quot;, &quot;login.microsoftonline.com&quot;, &quot;python.org&quot;, &quot;pypi.org&quot;, &quot;pythonhosted.org&quot; ]   ","version":"Next","tagName":"h3"},{"title":"Missing Metastore ID Error‚Äã","type":1,"pageTitle":"Troubleshooting","url":"/terraform-databricks-sra/docs/usage/Azure/troubleshooting/#missing-metastore-id-error","content":" Error:  Error: If var.create_hub is false, you must provide databricks_metastore_id   Cause: You set create_hub = false but didn't provide an existing metastore ID.  Solution:  Provide the metastore ID from your existing hub:  create_hub = false databricks_metastore_id = &quot;your-metastore-id-here&quot;   To find your metastore ID, use the Databricks CLI or Azure portal.  ","version":"Next","tagName":"h3"},{"title":"Missing NCC or Network Policy ID Error‚Äã","type":1,"pageTitle":"Troubleshooting","url":"/terraform-databricks-sra/docs/usage/Azure/troubleshooting/#missing-ncc-or-network-policy-id-error","content":" Error:  Error: If create_hub is false, then you must provide existing_ncc_id Error: If create_hub is false, then you must provide existing_network_policy_id   Cause: You're using BYO hub mode but didn't provide the required NCC and network policy IDs.  Solution:  Provide both IDs from your existing hub:  create_hub = false existing_ncc_id = &quot;your-ncc-id&quot; existing_network_policy_id = &quot;your-network-policy-id&quot;   ","version":"Next","tagName":"h3"},{"title":"Missing CMK IDs Error‚Äã","type":1,"pageTitle":"Troubleshooting","url":"/terraform-databricks-sra/docs/usage/Azure/troubleshooting/#missing-cmk-ids-error","content":" Error:  Error: existing_cmk_ids must be provided when create_hub is false and cmk_enabled is true   Cause: You're using BYO hub mode with CMK enabled but didn't provide existing CMK IDs.  Solution 1: Provide existing CMK IDs  create_hub = false cmk_enabled = true existing_cmk_ids = { key_vault_id = &quot;/subscriptions/.../Microsoft.KeyVault/vaults/kv-hub&quot; managed_disk_key_id = &quot;https://kv-hub.vault.azure.net/keys/cmk-disk/abc123&quot; managed_services_key_id = &quot;https://kv-hub.vault.azure.net/keys/cmk-services/def456&quot; }   Solution 2: Disable CMK  If your organization doesn't require CMK:  create_hub = false cmk_enabled = false   ","version":"Next","tagName":"h3"},{"title":"Workspace VNET Configuration Error‚Äã","type":1,"pageTitle":"Troubleshooting","url":"/terraform-databricks-sra/docs/usage/Azure/troubleshooting/#workspace-vnet-configuration-error","content":" Error:  Error: workspace_vnet must be provided when create_workspace_vnet is true Error: existing_workspace_vnet must be provided when create_workspace_vnet is false   Cause: Mismatch between create_workspace_vnet setting and provided network configuration.  Solution for SRA-managed network:  create_workspace_vnet = true workspace_vnet = { cidr = &quot;10.0.4.0/22&quot; }   Solution for BYO network:  create_workspace_vnet = false existing_workspace_vnet = { network_configuration = { virtual_network_id = &quot;/subscriptions/.../virtualNetworks/vnet-spoke&quot; private_subnet_id = &quot;/subscriptions/.../subnets/container&quot; public_subnet_id = &quot;/subscriptions/.../subnets/host&quot; private_endpoint_subnet_id = &quot;/subscriptions/.../subnets/private-endpoints&quot; # ... (full configuration) } dns_zone_ids = { backend = &quot;/subscriptions/.../privateDnsZones/privatelink.azuredatabricks.net&quot; dfs = &quot;/subscriptions/.../privateDnsZones/privatelink.dfs.core.windows.net&quot; blob = &quot;/subscriptions/.../privateDnsZones/privatelink.blob.core.windows.net&quot; } }   ","version":"Next","tagName":"h3"},{"title":"CSP Standards Without Profile Enabled‚Äã","type":1,"pageTitle":"Troubleshooting","url":"/terraform-databricks-sra/docs/usage/Azure/troubleshooting/#csp-standards-without-profile-enabled","content":" Error:  Error: If a compliance standard is provided in var.workspace_security_compliance.compliance_security_profile_standards, var.workspace_security_compliance.compliance_security_profile_enabled must be true.   Cause: You specified compliance standards but didn't enable the compliance security profile.  Solution:  Enable the profile when specifying standards:  workspace_security_compliance = { compliance_security_profile_enabled = true # Required! compliance_security_profile_standards = [&quot;HIPAA&quot;] }     ","version":"Next","tagName":"h3"},{"title":"Network Issues‚Äã","type":1,"pageTitle":"Troubleshooting","url":"/terraform-databricks-sra/docs/usage/Azure/troubleshooting/#network-issues","content":" ","version":"Next","tagName":"h2"},{"title":"Classic Compute Cannot Access Internet‚Äã","type":1,"pageTitle":"Troubleshooting","url":"/terraform-databricks-sra/docs/usage/Azure/troubleshooting/#classic-compute-cannot-access-internet","content":" Symptom: Classic compute clusters cannot install packages or access external URLs. Errors like:  Could not reach pypi.org Connection timed out   Cause: The default SRA configuration has no internet access for security.  Solution:  Add required URLs to allowed_fqdns:  # For Python packages allowed_fqdns = [ &quot;python.org&quot;, &quot;*.python.org&quot;, &quot;pypi.org&quot;, &quot;*.pypi.org&quot;, &quot;pythonhosted.org&quot;, &quot;*.pythonhosted.org&quot; ]   # For R packages allowed_fqdns = [ &quot;cran.r-project.org&quot;, &quot;*.cran.r-project.org&quot;, &quot;r-project.org&quot; ]   See Network Egress Configuration for more details.  ","version":"Next","tagName":"h3"},{"title":"Serverless Cannot Access Internet (Hub Workspace)‚Äã","type":1,"pageTitle":"Troubleshooting","url":"/terraform-databricks-sra/docs/usage/Azure/troubleshooting/#serverless-cannot-access-internet-hub-workspace","content":" Symptom: Serverless compute in the hub workspace cannot install packages or access external URLs.  Cause: The default SRA configuration has no internet access for hub serverless.  Solution:  Add required URLs to hub_allowed_urls (no wildcards supported):  hub_allowed_urls = [ &quot;python.org&quot;, &quot;pypi.org&quot;, &quot;pythonhosted.org&quot; ]   ","version":"Next","tagName":"h3"},{"title":"Serverless Cannot Access Internet (Spoke Workspace)‚Äã","type":1,"pageTitle":"Troubleshooting","url":"/terraform-databricks-sra/docs/usage/Azure/troubleshooting/#serverless-cannot-access-internet-spoke-workspace","content":" Symptom: Serverless compute in spoke workspaces cannot access external URLs.  Cause: The default SRA configuration has no internet access for spoke serverless.  Solution:  Add required URLs to allowed_fqdns (wildcards supported):  allowed_fqdns = [ &quot;pypi.org&quot;, &quot;*.pypi.org&quot;, &quot;pythonhosted.org&quot;, &quot;*.pythonhosted.org&quot; ]     ","version":"Next","tagName":"h3"},{"title":"Terraform State Issues‚Äã","type":1,"pageTitle":"Troubleshooting","url":"/terraform-databricks-sra/docs/usage/Azure/troubleshooting/#terraform-state-issues","content":" ","version":"Next","tagName":"h2"},{"title":"Resource Already Exists‚Äã","type":1,"pageTitle":"Troubleshooting","url":"/terraform-databricks-sra/docs/usage/Azure/troubleshooting/#resource-already-exists","content":" Symptom:  Error: A resource with the ID &quot;...&quot; already exists Error: resource already exists and cannot be created   Cause: Resource exists in Azure but not in Terraform state (often from a previous failed deployment).  Solution:  Import the existing resource into Terraform state:  terraform import &lt;resource_type&gt;.&lt;resource_name&gt; &lt;azure_resource_id&gt;   Examples:  # Import resource group terraform import azurerm_resource_group.spoke \\ /subscriptions/&lt;sub-id&gt;/resourceGroups/rg-spoke # Import workspace terraform import module.spoke_workspace.azurerm_databricks_workspace.this \\ /subscriptions/&lt;sub-id&gt;/resourceGroups/&lt;rg-name&gt;/providers/Microsoft.Databricks/workspaces/&lt;workspace-name&gt;   Alternative: If the resource was from a previously failed deployment and can be safely deleted, you can delete the resource and retry terraform apply.    ","version":"Next","tagName":"h3"},{"title":"Getting More Help‚Äã","type":1,"pageTitle":"Troubleshooting","url":"/terraform-databricks-sra/docs/usage/Azure/troubleshooting/#getting-more-help","content":" If you encounter issues not covered here:  Check GitHub Issues: databricks/terraform-databricks-sra/issues Review Terraform Documentation: Databricks ProviderAzureRM Provider Enable Debug Logging: export TF_LOG=DEBUG terraform apply 2&gt;&amp;1 | tee terraform-debug.log Open a GitHub Issue with: Your deployment modeSanitized terraform.tfvars (remove sensitive values)Full error messageDebug logs (if applicable)Steps to reproduce ","version":"Next","tagName":"h2"}],"options":{"id":"default"}}