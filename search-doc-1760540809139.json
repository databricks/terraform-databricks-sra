{"searchDocs":[{"title":"Overview","type":0,"sectionRef":"#","url":"/terraform-databricks-sra/docs/overview/","content":"","keywords":"","version":"Next"},{"title":"Point-in-Time Solution‚Äã","type":1,"pageTitle":"Overview","url":"/terraform-databricks-sra/docs/overview/#point-in-time-solution","content":" The Security Reference Architecture (SRA) - Terraform Templates is designed as a point-in-time solution that captures security best practices at the time of each release. This project does not guarantee backward compatibility between versions; new releases are not drop-in replacements for existing codebases.  ","version":"Next","tagName":"h2"},{"title":"Project Support‚Äã","type":1,"pageTitle":"Overview","url":"/terraform-databricks-sra/docs/overview/#project-support","content":" The code in this project is provided for exploration purposes only and is not formally supported by Databricks under any Service Level Agreements (SLAs). It is provided AS-IS, without any warranties or guarantees. Please do not submit support tickets to Databricks for issues related to the use of this project. The source code provided is subject to the Databricks LICENSE . All third-party libraries included or referenced are subject to their respective licenses set forth in the project license. Any issues or bugs found should be submitted as GitHub Issues on the project repository. While these will be reviewed as time permits, there are no formal SLAs for support. ","version":"Next","tagName":"h2"},{"title":"Usage","type":0,"sectionRef":"#","url":"/terraform-databricks-sra/docs/usage/","content":"Usage Note For feedback, questions, and comments ‚Äî please open a GitHub Issue. Please review the Project Support section for important information on support and service terms. The Security Reference Architecture (SRA) can be used to deploy Databricks on any of the supported cloud platforms: AWS/AWS GovCloudAzureGCP Please follow the usage guide specific to your cloud provider.","keywords":"","version":"Next"},{"title":"AWS","type":0,"sectionRef":"#","url":"/terraform-databricks-sra/docs/usage/AWS/","content":"","keywords":"","version":"Next"},{"title":"Overview‚Äã","type":1,"pageTitle":"AWS","url":"/terraform-databricks-sra/docs/usage/AWS/#overview","content":" Note For feedback, questions, and comments ‚Äî please open a GitHub Issue. Please review the Project Support section for important information on support and service terms.  The Security Reference Architecture (SRA) with Terraform provides a prescriptive deployment pattern for Databricks on AWS and AWS GovCloud, designed for highly secure and regulated environments. It captures Databricks security best practices in Terraform templates, allowing organizations to programmatically deploy workspaces and supporting infrastructure with hardened, opinionated defaults.  This architecture emphasizes:  Secure by Default ‚Äì Environments are provisioned with restrictive networking, scoped IAM roles, and encryption controls.Governance &amp; Compliance ‚Äì Aligns with industry standards and simplifies audits through consistent, automated configurations.Flexibility ‚Äì Templates can be customized to integrate with existing networking and security controls.Point-in-Time Design ‚Äì Each release reflects security best practices at that time; new releases may not be drop-in replacements.  ","version":"Next","tagName":"h2"},{"title":"Architecture Diagram‚Äã","type":1,"pageTitle":"AWS","url":"/terraform-databricks-sra/docs/usage/AWS/#architecture-diagram","content":"   The AWS implementation of the Security Reference Architecture (SRA) includes:  Configurable networking patterns (isolated or custom).Core AWS components such as VPCs and privatelink endpoints, IAM roles, S3 buckets, and KMS keys.Core Databricks components including Unity Catalog, system tables, audit log delivery, and network connectivty configurations and network policies for serverless compute.  ","version":"Next","tagName":"h2"},{"title":"Next Steps‚Äã","type":1,"pageTitle":"AWS","url":"/terraform-databricks-sra/docs/usage/AWS/#next-steps","content":" Review the Read Before Deploying section for critical considerations.Explore the SRA Components Breakdown to understand the included AWS and Databricks resources.Follow the Getting Started guide to deploy using Terraform. ","version":"Next","tagName":"h2"},{"title":"Azure","type":0,"sectionRef":"#","url":"/terraform-databricks-sra/docs/usage/Azure/","content":"","keywords":"","version":"Next"},{"title":"Overview‚Äã","type":1,"pageTitle":"Azure","url":"/terraform-databricks-sra/docs/usage/Azure/#overview","content":" Note For feedback, questions, and comments ‚Äî please open a GitHub Issue. Please review the Project Support section for important information on support and service terms.  The Security Reference Architecture (SRA) with Terraform provides a prescriptive deployment pattern for Databricks on Azure, designed for highly secure and regulated environments. It captures Databricks security best practices in Terraform templates, allowing organizations to programmatically deploy workspaces and supporting infrastructure with hardened, opinionated defaults.  This architecture emphasizes:  Secure by Default ‚Äì Environments are provisioned with restrictive networking, managed identites, and encryption controls.Governance &amp; Compliance ‚Äì Aligns with industry standards and simplifies audits through consistent, automated configurations.Scalability ‚Äì Built on a hub-and-spoke model to isolate workloads, enforce least privilege, and simplify cross-environment connectivity.Point-in-Time Design ‚Äì Each release reflects security best practices at that time; new releases may not be drop-in replacements.  ","version":"Next","tagName":"h2"},{"title":"Architecture Diagram‚Äã","type":1,"pageTitle":"Azure","url":"/terraform-databricks-sra/docs/usage/Azure/#architecture-diagram","content":"   The Azure implementation of the Security Reference Architecture (SRA) includes:  Hub-and-Spoke model for network segmentation, workload isolation, and to simplify cross-environment connectivity.Core Azure components such as Vnet Injection, privatelink endpoints, managed identities, Azure Storage Accounts, and Azure Key Vault.Core Databricks components including Unity Catalog, network connectivty configurations, and network policies for serverless compute.  ","version":"Next","tagName":"h2"},{"title":"Next Steps‚Äã","type":1,"pageTitle":"Azure","url":"/terraform-databricks-sra/docs/usage/Azure/#next-steps","content":" Review the Read Before Deploying section for critical considerations.Explore the SRA Components Breakdown to understand the included Azure and Databricks resources.Follow the Getting Started guide to deploy using Terraform. ","version":"Next","tagName":"h2"},{"title":"Additional Resources","type":0,"sectionRef":"#","url":"/terraform-databricks-sra/docs/additionalresources/","content":"Additional Resources Learn more about Databricks Security and Trust best practices and the Security Reference Architecture (SRA) through these resources. üõ°Ô∏è Databricks Security Databricks Security &amp; Trust CenterSecurity Best Practices for Databricks on AWSSecurity Best Practices for Azure DatabricksSecurity Best Practices for Databricks on GCPDatabricks Security Best Practices YouTube SeriesSecurity Analysis Tool (SAT) üìù Databricks Platform Blog Posts A Unified Approach to Data Exfiltration Protection on DatabricksData Exfiltration Protection with Databricks Lakehouse Platform on AWSData Exfiltration Protection with Azure DatabricksDatabricks on GCP - A practitioners guide on data exfiltration protections üîó Useful Links Databricks DocumentationDatabricks Platform SME Medium PageAdditional Databricks Terraform ExamplesDatabricks Terraform Provider DocumentationTerraform Documentation","keywords":"","version":"Next"},{"title":"SRA Components Breakdown","type":0,"sectionRef":"#","url":"/terraform-databricks-sra/docs/usage/AWS/components/","content":"","keywords":"","version":"Next"},{"title":"Network Configuration‚Äã","type":1,"pageTitle":"SRA Components Breakdown","url":"/terraform-databricks-sra/docs/usage/AWS/components/#network-configuration","content":" Two network configurations are available for workspaces: Isolated or Custom:  Isolated (Default): Restricts all traffic from reaching the public internet. Communication is limited to AWS PrivateLink endpoints for AWS services and the Databricks control plane. Note: A Unity Catalog‚Äìonly configuration is required for clusters running without internet access. Read the offical documentation for details. Custom: Enables specification of a VPC ID, subnet IDs, security group IDs, and Databricks PrivateLink endpoint IDs. This option is recommended when networking assets are provisioned in separate pipelines or pre-assigned by a centralized infrastructure team.  ","version":"Next","tagName":"h2"},{"title":"Core AWS Components‚Äã","type":1,"pageTitle":"SRA Components Breakdown","url":"/terraform-databricks-sra/docs/usage/AWS/components/#core-aws-components","content":" Customer Managed VPC: A customer-managed VPC provides full control over network configurations to meet organizational cloud security and governance standards.S3 Buckets: Three S3 buckets are created to support the following functionalities: Workspace Root BucketUnity Catalog - Workspace CatalogAudit Log Delivery Bucket IAM Roles: Three IAM roles are created to support the following functionalities: Classic Compute (EC2) ProvisioningData Access for Unity Catalog - Workspace CatalogAudit Log Delivery Scoped-down IAM Policy for the Databricks Cross-Account Role: A cross-account role is required for clusters provisioned within the classic compute plane. The role is scoped to the VPC, subnets, and security group associated with the deployment.AWS VPC Endpoints for S3 Gateway, STS, and Kinesis: AWS PrivateLink is used to connect the VPC to AWS services without traversing public IP addresses. S3 Gateway, STS, and Kinesis endpoints are best practices for enterprise Databricks deployments. Additional endpoints such as those for AWS DynamoDB and AWS Glue can be configured based on your use cases. NOTE: In the Isolated network mode, restrictive VPC endpoint policies are applied for S3, STS, and Kinesis. These must be updated if additional access is required through the classic compute plane. Back-end AWS PrivateLink Connectivity: Ensures private communication between the classic compute plane and the Databricks control plane (Back-end PrivateLink) via Databricks-specific interface VPC endpoints. Front-end PrivateLink, which keeps user traffic on the AWS backbone, is available but not included in these templates.AWS KMS Keys: Three AWS KMS Keys are created for: Workspace StorageManaged ServicesUnity Catalog - Workspace Catalog  ","version":"Next","tagName":"h2"},{"title":"Core Databricks Components‚Äã","type":1,"pageTitle":"SRA Components Breakdown","url":"/terraform-databricks-sra/docs/usage/AWS/components/#core-databricks-components","content":" Unity Catalog: Unity Catalog is a unified governance solution for data and AI assets such as files, tables, and machine learning models. Unity Catalog enforces fine-grained access controls, centralized policy management, auditing, and lineage tracking‚Äîall integrated into the Databricks workflow.System Table Schemas: System Tables provide operational visibility across access, compute, Lakeflow, query, serving, and storage logs. These tables are located within the system catalog in Unity Catalog.Audit Log Delivery: Enables low-latency delivery of Databricks audit logs to an S3 bucket within the customer's AWS account. Audit Logs capture both workspace-level and account-level events, with an option to enable verbose logging for more detailed event data. NOTE: Audit log delivery can only be configured twice per account. Once enabled, set audit_log_delivery_exists = true for subsequent runs. Network Connectivity Configuration: Serverless network connectivity is managed with network connectivity configurations (NCC), which are account-level regional constructs that are used to manage private endpoints creation and firewall enablement at scale. An NCC is created and attached to the workspace, which contains a list of stable IP addresses, which will be used by the serverless compute in that workspace to connect to customer cloud resources.Restrictive Network Policy: Network Policies implement egress controls for serverless compute by enforcing a restrictive network policy that permits outbound traffic only to required data buckets.Example Classic Cluster: Includes a sample cluster and associated cluster policy to illustrate secure configuration patterns. NOTE: Deploying this example creates a cluster within the Databricks workspace, including the underlying AWS EC2 instance. ","version":"Next","tagName":"h2"},{"title":"Read Before Deploying","type":0,"sectionRef":"#","url":"/terraform-databricks-sra/docs/usage/GCP/preread/","content":"","keywords":"","version":"Next"},{"title":"Key Considerations‚Äã","type":1,"pageTitle":"Read Before Deploying","url":"/terraform-databricks-sra/docs/usage/GCP/preread/#key-considerations","content":" The Security Reference Architecture (SRA) is a purpose-built, simplified deployment pattern designed for highly secure and regulated customers.  This architecture includes specific functionalities that may affect certain use cases, as outlined below.  Private Service Connect (PSC) Dependencies: This deployment implements back-end Private Service Connect (PSC) to ensure all workspace-to-control-plane traffic stays on the Google Cloud backbone. DNS records are automatically configured through Private Cloud DNS zones.The template does not include front-end PSC (for user-to-workspace traffic). If your security posture requires full private connectivity for user access, front-end PSC must be configured separately following the Databricks documentation. Customer-Managed VPC Configuration: The workspace is deployed into a Customer-Managed VPC to provide control over routing, firewall rules, and subnet segmentation. Ensure your organization‚Äôs security team reviews VPC peering or Shared VPC configurations before deployment.Adjust subnet CIDR ranges and routes as needed to align with your network design standards. Cloud KMS Integration: Customer-managed encryption keys (CMEK) are created via Cloud KMS and attached to both managed services and workspace storage.  ","version":"Next","tagName":"h2"},{"title":"Customizations‚Äã","type":1,"pageTitle":"Read Before Deploying","url":"/terraform-databricks-sra/docs/usage/GCP/preread/#customizations","content":" Terraform customizations are available to support the baseline deployment of the Security Reference Architecture (SRA).  These extensions and examples can be found in the top-level examples folder. ","version":"Next","tagName":"h2"},{"title":"Getting Started","type":0,"sectionRef":"#","url":"/terraform-databricks-sra/docs/usage/GCP/gettingstarted/","content":"","keywords":"","version":"Next"},{"title":"SRA Installation and Deployment Steps‚Äã","type":1,"pageTitle":"Getting Started","url":"/terraform-databricks-sra/docs/usage/GCP/gettingstarted/#sra-installation-and-deployment-steps","content":" Follow the steps below to deploy the Security Reference Architecture (SRA) using Terraform:  Clone the SRA repository.Install Terraform.Navigate to the gcp -&gt; examples folder and choose the deployment model to be used (byo_gcp_workspace_deployment or simple_workspace_deployment) and open up the *.tfvars.example file. Fill in the required values for all of the variables and relevant feature flags that are required for the deployment.Rename the file to terraform.tfvars. From the terminal, ensure you are in the correct working directory for the tf folder.Run terraform init.Run terraform validate.Run terraform plan.Run terraform apply.  NOTE: When deploying the workspace module, you must set the DATABRICKS_GOOGLE_SERVICE_ACCOUNT environment variable to the Service Account email that will be used for authentication.  Example:  export DATABRICKS_GOOGLE_SERVICE_ACCOUNT=&lt;&lt;Your GCP Service Account Email&gt;&gt;   ","version":"Next","tagName":"h2"},{"title":"Critical Next Steps‚Äã","type":1,"pageTitle":"Getting Started","url":"/terraform-databricks-sra/docs/usage/GCP/gettingstarted/#critical-next-steps","content":" The following steps outline essential security configurations that should be implemented after the initial deployment to further harden and operationalize the Databricks environment.  Implement a Front-End Mitigation Strategy: IP Access Lists: IP Access Lists enhance security by providing control over which networks can connect to your GCP Databricks account and workspaces.Front-End PrivateLink: Front-End Private Link establishes a private connection to the Databricks web application over the GCP backbone, preventing exposure to the public internet.  ","version":"Next","tagName":"h2"},{"title":"Additional Security recommendations‚Äã","type":1,"pageTitle":"Getting Started","url":"/terraform-databricks-sra/docs/usage/GCP/gettingstarted/#additional-security-recommendations","content":" Segment Workspaces for Data Separation: Use distinct workspaces for different teams or functions (e.g., security, marketing) to enforce data access boundaries and reduce risk exposure.Avoid Storing Production Datasets in Databricks File Store (DBFS): The DBFS root is accessible to all users in a workspace. Use external storage locations for production data and databases to ensure proper access control and auditing.Back Up Assets from the Databricks Control Plane: Regularly export and back up notebooks, jobs, and configurations using tools such as the Databricks Terraform Exporter.Regularly Restart Classic Compute Clusters: Restart clusters periodically to ensure the latest compute images and security patches are applied. Databricks recommends that admins restart clusters manually during a scheduled maintenance window to minimze the risk of disrupting a scheduled job or workflows.Implement a Tagging Strategy: Cluster and pool tags enable organizations to monitor costs and accurately attribute Databricks usage to specific business units or teams. These tags propagate to detailed DBU usage reports, supporting cost analysis and internal chargeback processes.Integrate CI/CD and Code Management: Evaluate workflow needs for Git-based version control and CI/CD automation. Incorporate code scanning, permission enforcement, and secret detection to enhance governance and operational efficiency.Run and Monitor the Security Analyis Tool (SAT): SAT analyzes your Databricks account and workspace configurations, providing recommendations to help you follow Databricks' security best practices. ","version":"Next","tagName":"h2"},{"title":"AWS GovCloud","type":0,"sectionRef":"#","url":"/terraform-databricks-sra/docs/usage/AWS/awsgovcloud/","content":"","keywords":"","version":"Next"},{"title":"Overview‚Äã","type":1,"pageTitle":"AWS GovCloud","url":"/terraform-databricks-sra/docs/usage/AWS/awsgovcloud/#overview","content":" Databricks supports deployments in AWS GovCloud (US) regions to meet the compliance, security, and data residency requirements of U.S. government agencies and contractors. The Security Reference Architecture (SRA) Terraform templates have been extended to support GovCloud-specific configurations while maintaining the same security-first design principles as commercial regions.  GovCloud deployments provide a controlled environment that adheres to U.S. government security and regulatory standards such as FedRAMP High and DoD IL5. These deployments ensure that all data and metadata remain within the U.S., operated exclusively by screened U.S. personnel.  ","version":"Next","tagName":"h2"},{"title":"Configuration Requirements‚Äã","type":1,"pageTitle":"AWS GovCloud","url":"/terraform-databricks-sra/docs/usage/AWS/awsgovcloud/#configuration-requirements","content":" In addition to the steps outlined in the Getting Started page, the following parameters must be defined in the Terraform configuration (.tfvars) when deploying in a GovCloud region:  Region: Set region to us-gov-west-1.GovCloud Shard: Set databricks_gov_shard to either civilian or dod. Use civilian for most U.S. government agency workloads.Use dod for Department of Defense (DoD) environments. NOTE: The dod shard is restricted to customers with a .mil email address.  For all non-GovCloud (commercial) deployments, leave databricks_gov_shard set to null.  ","version":"Next","tagName":"h2"},{"title":"Networking and Access Considerations‚Äã","type":1,"pageTitle":"AWS GovCloud","url":"/terraform-databricks-sra/docs/usage/AWS/awsgovcloud/#networking-and-access-considerations","content":" Private Connectivity: GovCloud workspaces can be deployed with full private connectivity through AWS PrivateLink, ensuring no traffic traverses the public internet from the classic compute plane.Region Isolation: All data and metadata remain within the designated GovCloud region, ensuring compliance with government data residency requirements.IAM and Policy Controls: Follow the same IAM and endpoint policy guidance as commercial SRA deployments, while aligning with agency-specific security baselines.  ","version":"Next","tagName":"h2"},{"title":"Additional Resources‚Äã","type":1,"pageTitle":"AWS GovCloud","url":"/terraform-databricks-sra/docs/usage/AWS/awsgovcloud/#additional-resources","content":" Databricks on AWS GovCloud DocumentationCompliance Security Profile and Enhanced Security Monitoring ","version":"Next","tagName":"h2"},{"title":"Getting Started","type":0,"sectionRef":"#","url":"/terraform-databricks-sra/docs/usage/Azure/gettingstarted/","content":"","keywords":"","version":"Next"},{"title":"SRA Installation and Deployment Steps‚Äã","type":1,"pageTitle":"Getting Started","url":"/terraform-databricks-sra/docs/usage/Azure/gettingstarted/#sra-installation-and-deployment-steps","content":" Follow the steps below to deploy the Security Reference Architecture (SRA) using Terraform:  Clone the SRA repository.Install Terraform.Navigate to the azure -&gt; tf folder and open up the template.tfvars.example file. Fill in the required values for all of the variables and relevant feature flags that are required for the deployment.Rename the file to terraform.tfvars. From the terminal, ensure you are in the correct working directory for the tf folder.Run terraform init.Run terraform validate.Run terraform plan.Run terraform apply.  Provider Initialization with Azure CLI and Possible Errors  If using the Azure CLI Authentication, a possible error that can be encountered is:  Error: cannot create mws network connectivity config: io.jsonwebtoken.IncorrectClaimException: Expected iss claim to be: https://sts.windows.net/00000000-0000-0000-0000-000000000000/, but was: https://sts.windows.net/ffffffff-ffff-ffff-ffff-ffffffffffff/   This typically happens if you are running Terraform in a tenant where you are a guest user, or if you have multiple Azure accounts configured.  To resolve this error, set the Azure Tenant ID by exporting the ARM_TENANT_ID environment variable:  export ARM_TENANT_ID=&quot;00000000-0000-0000-0000-000000000000&quot;   Alternatively, you can set the tenant ID directly in the Databricks provider configuration (see the provider documentation for details).  You may also encounter errors like the following when Terraform begins provisioning workspace resources:  ‚ï∑ ‚îÇ Error: cannot read current user: Unauthorized access to Org: 0000000000000000 ‚îÇ ‚îÇ with module.sat[0].module.sat.data.databricks_current_user.me, ‚îÇ on .terraform/modules/sat.sat/terraform/common/data.tf line 1, in data &quot;databricks_current_user&quot; &quot;me&quot;: ‚îÇ 1: data &quot;databricks_current_user&quot; &quot;me&quot; {} ‚îÇ   To fix this error:  Log in to the newly created spoke workspace by clicking Launch Workspace in the Azure portal. Ensure this is done as the same user running Terraform. Alternatively, grant workspace admin permissions to the Terraform user after the first user launches the workspace.  ","version":"Next","tagName":"h2"},{"title":"Critical Next Steps‚Äã","type":1,"pageTitle":"Getting Started","url":"/terraform-databricks-sra/docs/usage/Azure/gettingstarted/#critical-next-steps","content":" The following steps outline essential security configurations that should be implemented after the initial deployment to further harden and operationalize the Databricks environment.  Implement a Front-End Mitigation Strategy: IP Access Lists: IP Access Lists enhance security by providing control over which networks can connect to your Azure Databricks account and workspaces.Front-End PrivateLink: Front-End PrivateLink establishes a private connection to the Databricks web application over the Azure backbone, preventing exposure to the public internet.  ","version":"Next","tagName":"h2"},{"title":"Additional Security Recommendations‚Äã","type":1,"pageTitle":"Getting Started","url":"/terraform-databricks-sra/docs/usage/Azure/gettingstarted/#additional-security-recommendations","content":" Segment Workspaces for Data Separation: Use distinct workspaces for different teams or functions (e.g., security, marketing) to enforce data access boundaries and reduce risk exposure.Avoid Storing Production Datasets in Databricks File Store (DBFS): The DBFS root is accessible to all users in a workspace. Use external storage locations for production data and databases to ensure proper access control and auditing.Back Up Assets from the Databricks Control Plane: Regularly export and back up notebooks, jobs, and configurations using tools such as the Databricks Terraform Exporter.Regularly Restart Classic Compute Clusters: Restart clusters periodically to ensure the latest compute images and security patches are applied. Databricks recommends that admins restart clusters manually during a scheduled maintenance window to minimze the risk of disrupting a scheduled job or workflows.Implement a Tagging Strategy: Cluster and pool tags enable organizations to monitor costs and accurately attribute Databricks usage to specific business units or teams. These tags propagate to detailed DBU usage reports, supporting cost analysis and internal chargeback processes.Integrate CI/CD and Code Management: Evaluate workflow needs for Git-based version control and CI/CD automation. Incorporate code scanning, permission enforcement, and secret detection to enhance governance and operational efficiency.Run and Monitor the Security Analyis Tool (SAT): SAT analyzes your Databricks account and workspace configurations, providing recommendations to help you follow Databricks' security best practices. ","version":"Next","tagName":"h2"},{"title":"GCP","type":0,"sectionRef":"#","url":"/terraform-databricks-sra/docs/usage/GCP/","content":"","keywords":"","version":"Next"},{"title":"Overview‚Äã","type":1,"pageTitle":"GCP","url":"/terraform-databricks-sra/docs/usage/GCP/#overview","content":" Note For feedback, questions, and comments ‚Äî please open a GitHub Issue. Please review the Project Support section for important information on support and service terms.  The Security Reference Architecture (SRA) with Terraform provides a prescriptive deployment pattern for Databricks on GCP, designed for highly secure and regulated environments. It captures Databricks security best practices in Terraform templates, allowing organizations to programmatically deploy workspaces and supporting infrastructure with hardened, opinionated defaults.  This architecture emphasizes:  Secure by Default ‚Äì Deployments leverage customer-managed VPCs, Private Service Connect (PSC), and customer-managed encryption keys (CMEK) to ensure all data plane and control plane communication remains within Google‚Äôs private network and under customer control.Governance &amp; Compliance ‚Äì esigned to meet stringent enterprise and regulatory requirements by standardizing secure network boundaries, key management, and private connectivity through reusable Terraform modules.Scalability ‚Äì Uses a modular network foundation that supports multiple workspaces across shared VPCs, enabling consistent security enforcement, resource isolation, and simplified lifecycle management.Point-in-Time Design ‚Äì Each release reflects security best practices at that time; new releases may not be drop-in replacements.  ","version":"Next","tagName":"h2"},{"title":"Architecture Diagram‚Äã","type":1,"pageTitle":"GCP","url":"/terraform-databricks-sra/docs/usage/GCP/#architecture-diagram","content":"   The GCP implementation of the Security Reference Architecture (SRA) includes:  A customer managed VPC model for centralized network management, workload isolation, and simplified multi-workspace connectivity.Core GCP components such as customer-managed VPCs, Private Service Connect (PSC) for back-end connectivity, Cloud KMS for customer-managed encryption keys (CMEK), and Private Cloud DNS for internal name resolution.  ","version":"Next","tagName":"h2"},{"title":"Next Steps‚Äã","type":1,"pageTitle":"GCP","url":"/terraform-databricks-sra/docs/usage/GCP/#next-steps","content":" Review the Read Before Deploying section for critical considerations.Explore the SRA Components Breakdown to understand the included GCP and Databricks resources.Follow the Getting Started guide to deploy using Terraform. ","version":"Next","tagName":"h2"},{"title":"SRA Components Breakdown","type":0,"sectionRef":"#","url":"/terraform-databricks-sra/docs/usage/GCP/components/","content":"","keywords":"","version":"Next"},{"title":"Core GCP Components‚Äã","type":1,"pageTitle":"SRA Components Breakdown","url":"/terraform-databricks-sra/docs/usage/GCP/components/#core-gcp-components","content":" Customer-Managed VPC: A Customer-Managed VPC provides full control over network configuration, routing, and segmentation. This setup is required for Private Service Connect (PSC) and enables centralized management of multiple workspaces within a single, compliant network environment.Back-End Private Service Connect (PSC): Establishes private connectivity between the Databricks control plane and the customer-managed VPC. This ensures that all control plane and compute plane communication remains on the Google Cloud backbone, eliminating public internet exposure. NOTE: Front-end PSC for user-to-workspace connectivity is not included in this deployment. Cloud KMS Keys (CMEK): Implements customer-managed encryption keys for both control plane‚Äìmanaged services (such as notebooks, secrets, and Databricks SQL query data) and workspace storage (including Cloud Storage buckets and GCE Persistent Disks).  After the workspace is created, the following components are provisioned:  Cloud DNS: Configures private DNS zones required for PSC to resolve internal Databricks service endpoints to private IPs within the VPC, maintaining isolation and secure communication between components. ","version":"Next","tagName":"h2"},{"title":"Read Before Deploying","type":0,"sectionRef":"#","url":"/terraform-databricks-sra/docs/usage/Azure/preread/","content":"","keywords":"","version":"Next"},{"title":"Key Considerations‚Äã","type":1,"pageTitle":"Read Before Deploying","url":"/terraform-databricks-sra/docs/usage/Azure/preread/#key-considerations","content":" The Security Reference Architecture (SRA) is a purpose-built, simplified deployment pattern designed for highly secure and regulated customers.  This architecture includes specific functionalities that may affect certain use cases, as outlined below.  Azure Firewall: Azure Firewall is deployed to securely manage and control outbound traffic from the classic compute plane. It provides centralized logging, monitoring, and policy enforcement while maintaining compliance with organizational security standards. To add packages to classic compute or serverless compute, set up a private repository for scanned packages or update firewall rules/network policies to specified domains. Hub-and-Spoke Network Topology: The architecture employs a hub-and-spoke model: The hub VNet contains shared infrastructure and services.Each spoke Vnet houses isolated Azure Databricks workspaces for different business units or teams. This topology enhances security by isolating workloads and controlling traffic flow between VNets. Private Connectivity: Azure Databricks workspaces are deployed with VNet injection and Private Link, ensuring that all traffic between the workspace and Azure services remains within the Azure backbone network. Private endpoints are configured for services like Azure Storage, Event Hubs, and SQL Databases to prevent data exfiltration. Isolated Unity Catalog Securables: Unity Catalog securables like catalogs, Storage Credentials, and External Locations are isolated to individual workspaces. To share securables between workspaces, update the resources using the databricks_workspace_binding resource. Security Analysis Tool (SAT): The Security Analysis Tool (SAT) is enabled by default to continuously monitor the security posture of your Databricks environment. By default, SAT is installed in the hub workspace. ","version":"Next","tagName":"h2"},{"title":"Read Before Deploying","type":0,"sectionRef":"#","url":"/terraform-databricks-sra/docs/usage/AWS/preread/","content":"","keywords":"","version":"Next"},{"title":"Key Considerations‚Äã","type":1,"pageTitle":"Read Before Deploying","url":"/terraform-databricks-sra/docs/usage/AWS/preread/#key-considerations","content":" The Security Reference Architecture (SRA) is a purpose-built, simplified deployment pattern designed for highly secure and regulated customers.  This architecture includes specific functionalities that may affect certain use cases, as outlined below.  No outbound internet traffic: There is no outbound internet access from the classic compute plane or serverless compute plane. To add packages to classic compute or serverless compute, set up a private repository for scanned packages.Consider using a modern firewall solution to connect to public API endpoints if public internet connectivity is required. Restrictive AWS Resource Policies: Restrictive bucket and endpoint policies have been implemented for the workspace root storage bucket, S3 gateway endpoint, and the STS and Kinesis interface endpoints. These restrictions are continuously refined as the product evolves. Policies can be adjusted to allow access to additional AWS resources, such as other S3 buckets.If you encounter unexpected product behavior due to a policy in this repository, please raise a GitHub issue. Isolated Unity Catalog Securables: Unity Catalog securables like catalogs, Storage Credentials, and External Locations are isolated to individual workspaces. To share securables between workspaces, update the resources using the databricks_workspace_binding resource.  ","version":"Next","tagName":"h2"},{"title":"Customizations‚Äã","type":1,"pageTitle":"Read Before Deploying","url":"/terraform-databricks-sra/docs/usage/AWS/preread/#customizations","content":" Terraform customizations are available to support the baseline deployment of the Security Reference Architecture (SRA). These customizations are organized by provider:  Workspace: Databricks workspace provider  These extensions can be found in the top-level customizations folder. ","version":"Next","tagName":"h2"},{"title":"Getting Started","type":0,"sectionRef":"#","url":"/terraform-databricks-sra/docs/usage/AWS/gettingstarted/","content":"","keywords":"","version":"Next"},{"title":"SRA Installation and Deployment Steps‚Äã","type":1,"pageTitle":"Getting Started","url":"/terraform-databricks-sra/docs/usage/AWS/gettingstarted/#sra-installation-and-deployment-steps","content":" Follow the steps below to deploy the Security Reference Architecture (SRA) using Terraform:  Clone the SRA repository.Install Terraform.Navigate to the aws -&gt; tf folder and open up the template.tfvars.example file. Fill in the required values for all of the variables and relevant feature flags that are required for the deployment.NOTE: If using custom mode for the network configuration, do not uncomment the variables for the isolated configuration. Simply uncomment the variables for the custom configuration and populate with the respective values for each.Rename the file to terraform.tfvars. Navigate to the provider.tf file and configure the AWS and Databricks Terraform provider authentication.From the terminal, ensure you are in the correct working directory for the tf folder.Run terraform init.Run terraform validate.Run terraform plan.Run terraform apply.  ","version":"Next","tagName":"h2"},{"title":"Critical Next Steps‚Äã","type":1,"pageTitle":"Getting Started","url":"/terraform-databricks-sra/docs/usage/AWS/gettingstarted/#critical-next-steps","content":" The following steps outline essential security configurations that should be implemented after the initial deployment to further harden and operationalize the Databricks environment.  Implement a Front-End Mitigation Strategy: IP Access Lists: Terraform configurations for enabling IP access lists are available in the customizations folder.Front-End PrivateLink: Establishes a private connection to the Databricks web application over the AWS backbone, preventing exposure to the public internet. Read the documentation here. Identity &amp; Access Management: Configure Single-Sign On and Multi-Factor Authentication: Enterprise deployments should implement SSO and MFA for secure authentication and identity management.Setup SCIM (System for Cross-domain Identity Management) Provisioning: For automated user and group provisioning, integrate SCIM through the Databricks account console.  ","version":"Next","tagName":"h2"},{"title":"Additional Security Recommendations‚Äã","type":1,"pageTitle":"Getting Started","url":"/terraform-databricks-sra/docs/usage/AWS/gettingstarted/#additional-security-recommendations","content":" The following recommendations help maintain a strong security posture across Databricks deployments. Some of these configurations extend beyond the SRA Terraform implementation and may require customer-specific setup (e.g., SCIM, SSO, or Front-End PrivateLink).  Segment Workspaces for Data Separation: Use distinct workspaces for different teams or functions (e.g., security, marketing) to enforce data access boundaries and reduce risk exposure.Avoid Storing Production Datasets in Databricks File Store (DBFS): The DBFS root is accessible to all users in a workspace. Use external storage locations for production data and databases to ensure proper access control and auditing.Back Up Assets from the Databricks Control Plane: Regularly export and back up notebooks, jobs, and configurations using tools such as the Databricks Terraform Exporter.Regularly Restart Classic Compute Clusters: Restart clusters periodically to ensure the latest compute images and security patches are applied.Integrate CI/CD and Code Management: Evaluate workflow needs for Git-based version control and CI/CD automation. Incorporate code scanning, permission enforcement, and secret detection to enhance governance and operational efficiency.Deploy and Run the Security Analyis Tool (SAT): SAT analyzes your Databricks account and workspace configurations, providing recommendations to help you follow Databricks' security best practices. ","version":"Next","tagName":"h2"},{"title":"SRA Components Breakdown","type":0,"sectionRef":"#","url":"/terraform-databricks-sra/docs/usage/Azure/components/","content":"","keywords":"","version":"Next"},{"title":"Core Azure Components‚Äã","type":1,"pageTitle":"SRA Components Breakdown","url":"/terraform-databricks-sra/docs/usage/Azure/components/#core-azure-components","content":" Vnet Injection: Vnet Injection allows Azure Databricks workspaces to be deployed directly into a customer-managed virtual network (VNet), providing control over network configuration to meet organizational security and governance requirements.Private Endpoints: Leveraging Azure Private Link, private endpoints connect the customer‚Äôs VNet to Azure services without using public IP addresses, ensuring secure, private communication.PrivateLink Connectivity: Private Link establishes private network paths between the customer‚Äôs data plane and the Databricks control plane, preventing traffic from traversing the public internet. This template configures Back-End Private Link for communication to the Databricks control plane from classic compute clusters.  ","version":"Next","tagName":"h2"},{"title":"Core Databricks Components‚Äã","type":1,"pageTitle":"SRA Components Breakdown","url":"/terraform-databricks-sra/docs/usage/Azure/components/#core-databricks-components","content":" Unity Catalog: Unity Catalog is a unified governance solution for data and AI assets such as files, tables, and machine learning models. Unity Catalog enforces fine-grained access controls, centralized policy management, auditing, and lineage tracking‚Äîall integrated into the Databricks workflow.Network Connectivity Configuration: Serverless network connectivity is managed with network connectivity configurations (NCC), which are account-level regional constructs that are used to manage private endpoints creation and firewall enablement at scale. An NCC is created and attached to the workspace, which contains a list of stable Azure service subnets, which will be used by the serverless compute in that workspace to connect the Azure resource using service endpoints.Restrictive Network Policy: Network Policies implement egress controls for serverless compute by enforcing a restrictive network policy that permits outbound traffic only to required data buckets.  ","version":"Next","tagName":"h2"},{"title":"Adding Additional Spokes‚Äã","type":1,"pageTitle":"SRA Components Breakdown","url":"/terraform-databricks-sra/docs/usage/Azure/components/#adding-additional-spokes","content":" To add additional spokes to this configuration, follow the steps below.  Add a New Key to the spoke config Variable  # Terraform variables (for example, terraform.tfvars) spoke_config = { spoke = { resource_suffix = &quot;spoke&quot; cidr = &quot;10.1.0.0/20&quot; tags = { environment = &quot;dev&quot; }, }, spoke_b = { # &lt;----- Add a new spoke config resource_suffix = &quot;spoke_b&quot; cidr = &quot;10.2.0.0/20&quot; tags = { environment = &quot;test&quot; } } }   Add a New Provider in providers.tf for the New Spoke  # providers.tf # New spoke provider provider &quot;databricks&quot; { alias = &quot;spoke_b&quot; host = module.spoke_b.workspace_url }   Copy the spoke.tf File: Copy spoke.tf to a new file (for example, spoke_b.tf).Modify the Module Names and References:  # spoke_b.tf module &quot;spoke_b&quot; { # &lt;----- Modify the name of the module to something unique source = &quot;./modules/spoke&quot; # Update these per spoke resource_suffix = var.spoke_config[&quot;spoke_b&quot;].resource_suffix vnet_cidr = var.spoke_config[&quot;spoke_b&quot;].cidr tags = var.spoke_config[&quot;spoke_b&quot;].tags depends_on = [module.hub] } module &quot;spoke_b_catalog&quot; { source = &quot;./modules/catalog&quot; # Update these per catalog for the catalog's spoke catalog_name = module.spoke_b.resource_suffix dns_zone_ids = [module.spoke_b.dns_zone_ids[&quot;dfs&quot;]] ncc_id = module.spoke_b.ncc_id resource_group_name = module.spoke_b.resource_group_name resource_suffix = module.spoke_b.resource_suffix subnet_id = module.spoke_b.subnet_ids.privatelink tags = module.spoke_b.tags providers = { databricks.workspace = databricks.spoke_b } }   Apply the Configuration: terraform apply.  ","version":"Next","tagName":"h2"},{"title":"Security Analysis Tool (SAT)‚Äã","type":1,"pageTitle":"SRA Components Breakdown","url":"/terraform-databricks-sra/docs/usage/Azure/components/#security-analysis-tool-sat","content":" The Security Analysis Tool (SAT) is enabled by default to continuously monitor the security posture of your Databricks environment. By default, SAT is installed in the hub workspace, also known as the WEB_AUTH workspace.  Changing the SAT Workspace  To deploy the Security Analysis Tool (SAT) in a different workspace, three modifications are required in customizations.tf:  Update the Databricks provider in the SAT module:  # Default providers = { databricks = databricks.hub } # Modified providers = { databricks = databricks.spoke }   Update the local sat_workspace reference:  # Default locals { sat_workspace = module.hub } # Modified locals { sat_workspace = module.spoke }   Update the databricks_permission_assignment.sat_workspace_admin resource  # Default resource &quot;databricks_permission_assignment&quot; &quot;sat_workspace_admin&quot; { count = length(module.sat) ... provider = databricks.hub } # Modified resource &quot;databricks_permission_assignment&quot; &quot;sat_workspace_admin&quot; { count = length(module.sat) ... provider = databricks.spoke }   NOTE: The Security Analysis Tool (SAT) is designed to be deployed once per Azure subscription. To deploy SAT in multiple regions, provision SAT in multiple spokes using the same modifications above.  SAT Service Principal  Some users may not have permissions to create Entra ID service principals. In this case, a pre-existing service principal can be used:  # example.tfvars sat_service_principal = { client_id = &quot;00000000-0000-0000-0000-000000000000&quot; client_secret = &quot;some-secret&quot; }   If no service principal is provided, the template creates one named spSAT by default. The name can be customized:  # example.tfvars sat_service_principal = { name = &quot;spSATDev&quot; }   SAT Compute  The Security Analysis Tool (SAT) is installed using classic compute by default. This is because SAT does not yet support inspecting workspaces outside of the current workspace when running on serverless. If you would like to run on serverless compute instead, you can modify the sat_configuration variable to specify using serverless (see below):  sat_configuration = { run_on_serverless = true }   NOTE: When running the Security Analysis Tool (SAT) on serverless compute, SAT will only inspect the current workspace. ","version":"Next","tagName":"h2"}],"options":{"id":"default"}}